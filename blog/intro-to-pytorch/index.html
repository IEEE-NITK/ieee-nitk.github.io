<!DOCTYPE html>
<html lang="pt-br">
        <head>
        <!-- Give a small description in 3-4 sentences about the website -->
        <meta name="abstract" content="IEEE NITK Student Chapter under Mangalore SubSection and Bangalore Section was founded in 1988. Currently it has three SIGs namely Computer Society, Circuits and Systems, Signal Processing Society and Piston. It also has two Affinty Groups namely Special Interest Group in Humanitarian Technologies (SIGHT) and Women in Engineering (WiE)">
        <!-- Important Keywords to be noted in the website  -->
        <meta name="keywords" content="IEEE, IEEE NITK, NITK Surathkal, NITK, Clubs in NITK, Technical Clubs in NITK">
        <!-- Tell the spider to index the first page and other pages as well-->
        <meta name="robots" content="index, follow">    
        <!-- How often should spiders come back to your page -->
        <meta name="revisit-after" content="3 days">

        <!-- Copyright regarding the website -->
        <meta name="copyright" content="IEEE NITK">
        <!-- Tells Google Bot not to duplicate description -->
        <meta name="googlebot" content="noodp">
        <!-- Language for the website -->
        <meta name="language" content="English">  

        <!-- Web Author for the image -->
        <meta name="web_author" content="IEEE NITK">
        <meta name="author" content="Salman Shah">
        <!-- Email ID -->
        <meta name="contact" content="ieee@nitk.edu.in" />
        <!-- Email ID to reply to -->
        <meta name="reply-to" content="ieee@nitk.edu.in">

        <!-- Refers to distribution of the page -->
        <meta name="distribution" content="global">
        <!-- Generator or formatter tag -->
        <meta name="generator" content="Jekyll">
        <!-- Disallow spammers for the webpage -->
        <meta name="no-email-collection" content="http://www.metatags.info/nospamharvesting">
        <!-- Rating for the page -->
        <meta name="rating" content="general">
        <!-- Content Type for the page -->
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <!-- Fixing viewport on mobile views -->
        <meta name="viewport" content="width=device-width; initial-scale=1; maximum-scale=1.0">

        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">

        <meta name="description" content="A post on the basics of PyTorch">

        <!-- Google Authorship Markup -->
        <!-- Social: Twitter -->
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:site" content="@IEEE_NITK">
        <meta name="twitter:title" content="An Introduction to PyTorch">
        <meta name="twitter:description" content="A post on the basics of PyTorch">
        <meta property="twitter:image:src" content="http://ieee.nitk.ac.in/blog/assets/img/blog-image.png">

        <!-- Social: Facebook / Open Graph -->
        <meta property="fb:app_id" content="0011038251882641" />
        <meta property="og:url" content="http://ieee.nitk.ac.in/blog/intro-to-pytorch/" />
        <meta property="og:title" content="An Introduction to PyTorch">
        <meta property="og:image" content="http://ieee.nitk.ac.in/blog/assets/img/blog-image.png">
        <meta property="og:description" content="A post on the basics of PyTorch">
        <meta property="og:site_name" content="A blog about our findings and musings">

        <!-- Social: Google+ / Schema.org  -->
        <meta itemprop="name" content="An Introduction to PyTorch"/>
        <meta itemprop="description" content="A post on the basics of PyTorch">
        <meta itemprop="image" content="http://ieee.nitk.ac.in/blog/assets/img/blog-image.png"/>
        <meta name="msapplication-TileColor" content="#ffffff">
        <meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
        <meta name="theme-color" content="#ffffff">

        <!-- Windows 8 Tile Icons -->
        <meta name="application-name" content="IEEE-NITK Blog">
        <meta name="msapplication-TileColor" content="#0562DC">
        <meta name="msapplication-square70x70logo" content="smalltile.png" />
        <meta name="msapplication-square150x150logo" content="mediumtile.png" />
        <meta name="msapplication-wide310x150logo" content="widetile.png" />
        <meta name="msapplication-square310x310logo" content="largetile.png" />
        
        <!-- Android Lolipop Theme Color -->
        <meta name="theme-color" content="#0562DC">

        <link rel="author" href="https://plus.google.com/?rel=author">

        <!-- Favicon -->
        <link rel="shortcut icon" href="/blog/assets/img/icons/favicon.ico" type="image/x-icon" />
        <link rel="manifest" href="/blog/assets/img/icons/manifest.json">
        <!-- Include Font Awesome -->
        <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

        <title>An Introduction to PyTorch</title>

        <!-- highlighter theme -->
        <link rel="stylesheet" href="/blog/assets/css/monokai-sublime.css">

        <link rel="stylesheet" href="/blog/assets/css/main.css">
        <link rel="canonical" href="/blog/intro-to-pytorch/">
        <link rel="alternate" type="application/rss+xml" title="A blog about our findings and musings"
        href="http://ieee.nitk.ac.in/blog/feed.xml" />

        

        <!-- jQuery for highlighter -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-106590424-3"></script>
        <script src="/blog/assets/js/sidebar.js"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-106590424-3');
            $(document).ready(function(){
                loadArticles();
            });
        </script>

    </head>

    <body>
        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-email" viewBox="0 0 1024 1024"><path class="path1" d="M950.857 859.429v-438.857q-18.286 20.571-39.429 37.714-153.143 117.714-243.429 193.143-29.143 24.571-47.429 38.286t-49.429 27.714-58.571 14h-1.143q-27.429 0-58.571-14t-49.429-27.714-47.429-38.286q-90.286-75.429-243.429-193.143-21.143-17.143-39.429-37.714v438.857q0 7.429 5.429 12.857t12.857 5.429h841.143q7.429 0 12.857-5.429t5.429-12.857zM950.857 258.857v-14t-0.286-7.429-1.714-7.143-3.143-5.143-5.143-4.286-8-1.429h-841.143q-7.429 0-12.857 5.429t-5.429 12.857q0 96 84 162.286 110.286 86.857 229.143 181.143 3.429 2.857 20 16.857t26.286 21.429 25.429 18 28.857 15.714 24.571 5.143h1.143q11.429 0 24.571-5.143t28.857-15.714 25.429-18 26.286-21.429 20-16.857q118.857-94.286 229.143-181.143 30.857-24.571 57.429-66t26.571-75.143zM1024 237.714v621.714q0 37.714-26.857 64.571t-64.571 26.857h-841.143q-37.714 0-64.571-26.857t-26.857-64.571v-621.714q0-37.714 26.857-64.571t64.571-26.857h841.143q37.714 0 64.571 26.857t26.857 64.571z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-rss" viewBox="0 0 805 1024"><path class="path1" d="M219.429 768q0 45.714-32 77.714t-77.714 32-77.714-32-32-77.714 32-77.714 77.714-32 77.714 32 32 77.714zM512 838.286q1.143 16-9.714 27.429-10.286 12-26.857 12h-77.143q-14.286 0-24.571-9.429t-11.429-23.714q-12.571-130.857-105.429-223.714t-223.714-105.429q-14.286-1.143-23.714-11.429t-9.429-24.571v-77.143q0-16.571 12-26.857 9.714-9.714 24.571-9.714h2.857q91.429 7.429 174.857 46t148 103.714q65.143 64.571 103.714 148t46 174.857zM804.571 839.429q1.143 15.429-10.286 26.857-10.286 11.429-26.286 11.429h-81.714q-14.857 0-25.429-10t-11.143-24.286q-6.857-122.857-57.714-233.429t-132.286-192-192-132.286-233.429-58.286q-14.286-0.571-24.286-11.143t-10-24.857v-81.714q0-16 11.429-26.286 10.286-10.286 25.143-10.286h1.714q149.714 7.429 286.571 68.571t243.143 168q106.857 106.286 168 243.143t68.571 286.571z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol><symbol id="icon-angle-down" viewBox="0 0 658 1024"><path class="path1" d="M614.286 420.571q0 7.429-5.714 13.143l-266.286 266.286q-5.714 5.714-13.143 5.714t-13.143-5.714l-266.286-266.286q-5.714-5.714-5.714-13.143t5.714-13.143l28.571-28.571q5.714-5.714 13.143-5.714t13.143 5.714l224.571 224.571 224.571-224.571q5.714-5.714 13.143-5.714t13.143 5.714l28.571 28.571q5.714 5.714 5.714 13.143z"/></symbol><symbol id="icon-github-alt" viewBox="0 0 951 1024"><path class="path1" d="M365.714 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM731.429 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM822.857 694.857q0-68.571-39.429-116.571t-106.857-48q-23.429 0-111.429 12-40.571 6.286-89.714 6.286t-89.714-6.286q-86.857-12-111.429-12-67.429 0-106.857 48t-39.429 116.571q0 50.286 18.286 87.714t46.286 58.857 69.714 34.286 80 16.857 85.143 4h96q46.857 0 85.143-4t80-16.857 69.714-34.286 46.286-58.857 18.286-87.714zM950.857 594.286q0 118.286-34.857 189.143-21.714 44-60.286 76t-80.571 49.143-97.143 27.143-98 12.571-95.429 2.571q-44.571 0-81.143-1.714t-84.286-7.143-87.143-17.143-78.286-29.429-69.143-46.286-49.143-65.714q-35.429-70.286-35.429-189.143 0-135.429 77.714-226.286-15.429-46.857-15.429-97.143 0-66.286 29.143-124.571 61.714 0 108.571 22.571t108 70.571q84-20 176.571-20 84.571 0 160 18.286 60-46.857 106.857-69.143t108-22.286q29.143 58.286 29.143 124.571 0 49.714-15.429 96 77.714 91.429 77.714 227.429z"/></symbol></defs></svg>

        <header class="header-post" role="banner">
    <div class="content">
        
            <time itemprop="datePublished" datetime="2019-09-27 21:14:44 +0530" class="date">27 Sep 2019</time>
        
        <h1 class="post-title" itemprop="name">An Introduction to PyTorch</h1>
        <p itemprop="description" class="subtitle">A post on the basics of PyTorch</p>
    </div>
     <a class="down" data-scroll href="#scroll"><svg class="icon icon-angle-down"><use xlink:href="#icon-angle-down"></use></svg></a>
     <div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search...">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>

<div id="fade" class="overlay"></div>
<a id="slide" class="slideButton fade">
    <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    <svg id="close" class="icon-menu"><use xlink:href="#icon-close"></use></svg>
</a>
<aside id="sidebar">
<nav id="navigation">
  <h2>MENU</h2>
  <ul>
    
    
      <li><a href="http://ieee.nitk.ac.in/blog">Home</a></li>
    
    
    
      <li><a href="http://ieee.nitk.ac.in/blog/tags">Tags</a></li>
    
    
    
      <li><a href="http://ieee.nitk.ac.in/">Main Website</a></li>
    
    
    
      <li><a href="http://ieee.nitk.ac.in/gyan">Gyan</a></li>
    
    
    <li><a class="feed" href="http://ieee.nitk.ac.in/blog/feed.xml" title="Atom/RSS feed">Feed</a></li>
  </ul>
</nav>
</aside>
<a id="search" class="dosearch">
    <svg class="icon-menu icon-search"><use xlink:href="#icon-search"></use></svg>
</a>

</header>

        <section class="post" itemscope itemtype="http://schema.org/BlogPosting">

            <div class="content-box">
                <article role="article" id="scroll" class="post-content" itemprop="articleBody">
                    <p><a href="https://colab.research.google.com/drive/1odpZAS42UzVk16TGol_ta24DtyX1ZB2B"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></p>
<h1>An Introduction to PyTorch</h1>

<p>PyTorch was released in early 2017 by Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan and has been making a pretty big impact in the deep learning community. It's is a Python-based scientific computing package targeted to be a: (1) A replacement for NumPy to use the power of GPUs and (2) A deep learning research platform that provides maximum flexibility and speed. It's developed as an open source project by the Facebook AI Research team, and is being adopted by teams everywhere in industry and academia and is very comfortable to learn and use. It is based on the Torch library and has both a Python and C++ frontend(though the Python frontend is more 'polished'). PyTorch is also very pythonic, meaning, it feels more natural to use it if you already are a Python developer. </p>

<h2>What we'll see</h2>
<p><br />
In this post, we will first look at the basics of Tensors(which are the building blocks of anything you do using PyTorch), and operations on them. We will then have a look at gradients and how they are computed in PyTorch. Finally we shall build a simple Neural Network for the IRIS dataset using PyTorch</p>

<h2>Prerequisites</h2>
<p>Knowledge of Python(3.x) is required. Knowledge of NumPy will be useful but is not necessary. For the last part(of building a Neural Network), a basic understanding of a simple neural network is assumed</p>

<h2>Installation</h2>
<p>Details of installation may be found <a href="https://pytorch.org/get-started/locally/">here</a>. However, to start off with, I would recommend using <a href="https://pytorch.org/get-started/locally/">Google Colab</a> or <a href="https://azure.microsoft.com/en-us/develop/pytorch/">Microsoft Azure</a></p>

<p>We'll first have a look at the building blocks of PyTorch (most other deep learning libraries also) which are Tensors.</p>

<h2>Tensors</h2>

<p>All computations in PyTorch generally consist of operations on Tensors. Tensors can be thought of as a generalisation of vectors or matrices in 1 or more dimensions, or more simply, are like arrays in a programming language like C. In some cases tensors are used as a replacement for NumPy to use the power of GPUs A 1D Tensor is like a 1D array, 2D Tensor like a 2D array and so on. Let’s see how we can use them.</p>

<h4>Importing torch</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'1.1.0'
</code></pre></div></div>

<p>Tensors are available in the torch library as <a href="https://pytorch.org/docs/stable/tensors.html"><strong><tt>torch.Tensor</tt></strong></a>. It is like a multidimensional array which can have elements of a single datatype. Computations between tensors are allowed only if the tensors share the same data type(dtype).</p>

<p>First lets create a tensor <strong>from a python list</strong>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">myTensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">myTensor</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1, 2, 3])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">myTensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">myTensor1</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[1., 2., 3.],
        [4., 5., 6.]])
</code></pre></div></div>

<p>Above, we see that the function determines the data type based on the inputs. We may use different constructors to specify the data type we need as:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fltTensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">fltTensor</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1., 2., 3.])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">intTensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">intTensor</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1, 2, 3], dtype=torch.int32)
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">longTensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">longTensor</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1, 2, 3])
</code></pre></div></div>

<p>We may also achieve the above using the dtype attribute in torch.tensor:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">intTensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">int</span><span class="p">)</span>
<span class="n">intTensor1</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1, 2, 3], dtype=torch.int32)
</code></pre></div></div>

<p><strong>Note:</strong> There is a subtle difference between the functions <tt>torch.tensor</tt> and <tt>torch.Tensor</tt>. torch.Tensor is an alias to torch.FloatTensor whereas torch.tensor determines the data type based on the input.</p>

<p>For more information on datatypes, check <a href="https://pytorch.org/docs/stable/tensors.html">here</a>.</p>

<h4>To and From a NumPy array</h4>

<p>Converting a torch tensor to a numpy array and vice versa is very easy and hence makes it easy to access other libraries like Scikit-Learn and Matplotlib</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">arr</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1 2 3 4 5]
int32
&lt;class 'numpy.ndarray'&gt;
</code></pre></div></div>

<p>We can use <tt>torch.from_numpy</tt> or <tt>torch.as_tensor</tt>:</p>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="n">x</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1, 2, 3, 4, 5], dtype=torch.int32)
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="nb">type</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;class 'torch.Tensor'&gt;
torch.IntTensor
</code></pre></div></div>

<p>Note that we can also use torch.tensor for this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="n">x1</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1, 2, 3, 4, 5], dtype=torch.int32)
</code></pre></div></div>

<p>The difference between <tt>torch.tensor</tt> and <tt>torch.from_numpy</tt>(or <tt>torch.as_tensor</tt>) is that when we use the former, a copy of the original tensor is made and stored in <tt>x1</tt>(above). Any changes made to <tt>x1</tt> will not affect <tt>arr</tt>(the numpy array) and vice versa. However, when we use the latter function, the tensor created (x in the example above) points to the same location in memory as does <tt>arr</tt>. Hence any changes done to <tt>x</tt> will also affect arr(the numpy array) and vice versa.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Using torch.from_numpy()
</span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="n">arr</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">=</span><span class="mi">100</span>
<span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([  0,   1, 100,   3,   4], dtype=torch.int32)
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Using torch.tensor()
</span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="n">arr</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">=</span><span class="mi">100</span>
<span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0, 1, 2, 3, 4], dtype=torch.int32)
</code></pre></div></div>

<h4>Creating special types of tensors</h4>

<p>Uninitialized tensors using <a href="https://pytorch.org/docs/stable/torch.html#torch.empty"><strong>torch.empty()</strong></a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])
</code></pre></div></div>

<p>Initialised with zeroes or ones using <a href="https://pytorch.org/docs/stable/torch.html#torch.zeros"><strong>torch.zeros()</strong></a> and <a href="https://pytorch.org/docs/stable/torch.html#torch.ones"><strong>torch.ones()</strong></a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Passing datatype is recommended but not compulsory
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
</code></pre></div></div>

<p>Tensors in a range using <a href="https://pytorch.org/docs/stable/torch.html#torch.arange"><strong>torch.arange(start,end,step)</strong></a> and <a href="https://pytorch.org/docs/stable/torch.html#torch.linspace"><strong>torch.linspace(start,end,number_of_elements)</strong></a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 0 included, 50 excluded
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 0,  5],
        [10, 15],
        [20, 25],
        [30, 35],
        [40, 45]])
</code></pre></div></div>

<p>Tensor to create 12 linearly spaced elements between 0 and 50 both included</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> 
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 0.0000,  4.5455,  9.0909, 13.6364],
        [18.1818, 22.7273, 27.2727, 31.8182],
        [36.3636, 40.9091, 45.4545, 50.0000]])
</code></pre></div></div>

<p>A seed for random numbers can be set using torch.manual_seed().</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;torch._C.Generator at 0x1bb6e171030&gt;
</code></pre></div></div>

<p>Generating random tensors:</p>

<p>A tensor of shape (3, 4) with random numbers from a uniform distribution over [0, 1)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> 
<span class="n">x</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0.9712, 0.0742, 0.5130, 0.7472],
        [0.4507, 0.9223, 0.9148, 0.1624],
        [0.7780, 0.1663, 0.6665, 0.4992]])
</code></pre></div></div>

<p>A tensor with shape (3, 4) with numbers from the normal distribution with mean 0 and standard deviation 1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> 
<span class="n">x</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 0.5252,  2.0810,  1.5700, -0.1474],
        [-0.2024,  0.4377,  1.1986,  0.7179],
        [-0.4969,  0.8618, -0.2603, -1.1157]])
</code></pre></div></div>

<p>A tensor of shape (3, 4) with random integers between 0 and 10</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span> 
<span class="n">x</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[7, 7, 1, 4],
        [7, 4, 9, 5],
        [1, 2, 5, 6]])
</code></pre></div></div>

<p>Instead of specifying the sizes of the tensors, we can use three other functions which serve the same purpose as above nute take in other tensors as inputs and return tensors of their shapes. Just suffix _like to the above functions as below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> 
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
<span class="n">y</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0.3099, 0.0135, 0.2955, 0.8752],
        [0.7608, 0.7589, 0.2097, 0.4063],
        [0.6469, 0.3655, 0.3926, 0.6284]])
</code></pre></div></div>

<p>Similarly <tt>torch.randn_like(x), torch.randint_like(0,10,x), torch.zeros_like(x)</tt> and <tt>torch.ones_like(x)</tt> may also be used.</p>

<h2>Operations on Tensors</h2>

<p>Now we will look at a few basic operations on Tensors. Indexing and slicing tensors are very similar to those of python lists. We shall look at a few examples</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0, 1],
        [2, 3],
        [4, 5]])
</code></pre></div></div>

<p>To get the left column:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0, 2, 4])
</code></pre></div></div>

<p>To get the left column as a (3,1) slice:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">[:,:</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0],
        [2],
        [4]])
</code></pre></div></div>

<p><strong>Reshaping a Tensor</strong></p>

<p>Two functions are generally used for reshaping tensor which are .view() and .reshape(). Both functions return a reshaped tensor without affecting the original tensor.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">x</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0, 1],
        [2, 3],
        [4, 5],
        [6, 7],
        [8, 9]])
</code></pre></div></div>

<p>However, we see that x is unchanged</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> 
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0, 1],
        [2, 3],
        [4, 5],
        [6, 7],
        [8, 9]])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="c1"># Unchanged
</span></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
</code></pre></div></div>

<p>To change the original tensor use <tt>x = x.reshape(2, 5)</tt></p>

<p>While .view() returns a tensor which shares storage with the original tensor, .reshape() may return a copy or a view of the original tensor. It (.reshape()) may or may not share the storage with the original tensor. Also .reshape() may act on <a href="https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays/26999092#26999092">contiguous</a> or non contiguous tensors, while .view can act only on contiguous tensors.</p>

<p>We can also infer the correct value for shape from the tensor by passing -1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0, 1, 2, 3, 4],
        [5, 6, 7, 8, 9]])
</code></pre></div></div>

<p>Also as seen before, we can suffix the function with ‘_as’ to pass in a tensor whose shape, we want to reshape the original tensor to.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 0,  2,  4,  6,  8],
        [10, 12, 14, 16, 18]])
</code></pre></div></div>

<p><strong>Other Basic Operations</strong></p>

<p>I will demonstrate the use of basic operations using the torch.add() function. This may be extended to other functions also.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([4., 6.])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([4., 6.])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">c</span><span class="p">)</span> <span class="c1">#Equivalent to c = torch.add(a, b)
</span></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([4., 6.])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="c1">#Equivalent tp a = torch.add(a,b)
</span><span class="n">a</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([4., 6.])
</code></pre></div></div>

<p>The above can be extended to all basic arithmetic operations. Now we will look at a few more operations</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Multiplication (element-wise)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([3., 8.])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Dot Product
</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(11.)
</code></pre></div></div>

<p>Now let us see matrix multiplication. Normal matrix multiplication (that we know of) can be done using <strong>torch.mm()</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">],[</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'a: '</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">'b: '</span><span class="p">,</span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">'a x b: '</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a:  torch.Size([2, 3])
b:  torch.Size([3, 2])
a x b:  torch.Size([2, 2])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[56., 62.],
        [80., 89.]])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">@</span> <span class="n">b</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[56., 62.],
        [80., 89.]])
</code></pre></div></div>

<p>Matrix multiplication can also be done with boradcasting using the <tt>torch.matmul()</tt> or <tt>@</tt> operator. Click <a href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics">here</a> for more details on broadcasting.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="k">print</span><span class="p">((</span><span class="n">a</span> <span class="o">@</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([2, 3, 5])
torch.Size([2, 3, 5])
</code></pre></div></div>

<p>But we see that there matrices are invalid for normal matrix multiplication</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">))</span> 
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-93-244d2942b50e&gt; in &lt;module&gt;
----&gt; 1 print(torch.mm(a,b))


RuntimeError: matrices expected, got 3D, 2D tensors at ..\aten\src\TH/generic/THTensorMath.cpp:956
</code></pre></div></div>

<p>Note that if the tensors satisfy the mathematical conditions of matric multiplication, then all the three above functions will be identical. It is however easier to detect errors using <tt>torch.mm()</tt> than the other two when the tensors are mathematically non-compatible and hence is recommended over the other two.</p>

<p><strong>Norm Function</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">8.</span><span class="p">,</span><span class="mf">14.</span><span class="p">])</span>
<span class="n">x</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(17.)
</code></pre></div></div>

<h2>Gradients in PyTorch</h2>

<p>PyTorch provides a module called ‘autograd’ to calculate the gradients of tensors automatically. It basically keeps track of all operations done on the tensor and backtracks along these operations to calculate gradients(or derivatives) along the way. To ensure that the operations are kept track of, we need to set the requires_grad attribute to True which can be done in two ways: (1) while creation, set the attribute to True as <tt>x = torch.arange(10, requires_grad = True)</tt> or (2) after creation, use <tt>x.requires_grad_(True)</tt>. The gradients are computed with respect to some variable y as <tt>y.backward()</tt>. This goes though all operations which were used to create y and calculates the gradients. For example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># Substitutes the value of x = 3 in the equation
</span></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(21., grad_fn=&lt;AddBackward0&gt;)
</code></pre></div></div>

<p><img src="/blog/assets/img/intro-to-pytorch/s1.png" alt="Explanation" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Perform backpropagation on y to calculate gradients
</span><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Display the gradient wrt x
</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(12.)
</code></pre></div></div>

<p><img src="/blog/assets/img/intro-to-pytorch/s2.png" alt="Explanation" /></p>

<p><strong>Calculating multi-level gradients</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[1., 2., 3.],
        [3., 2., 1.]], requires_grad=True)
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">3</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[5., 7., 9.],
        [9., 7., 5.]], grad_fn=&lt;AddBackward0&gt;)
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">z</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 50.,  98., 162.],
        [162.,  98.,  50.]], grad_fn=&lt;MulBackward0&gt;)
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(620., grad_fn=&lt;SumBackward0&gt;)
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>
<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[40., 56., 72.],
        [72., 56., 40.]])
</code></pre></div></div>

<p><img src="/blog/assets/img/intro-to-pytorch/s3.png" alt="Explanation" /></p>

<p><strong>Turning off tracking</strong></p>

<p>There may be times when we don’t want or need to track the computational history.</p>

<p>You can reset a tensor’s <tt>requires_grad</tt> attribute in-place using <tt>.requires_grad_(True)</tt> (or False) as needed.</p>

<p>When performing evaluations, it’s often helpful to wrap a set of operations in <tt>with torch.no_grad():</tt></p>

<p>A less-used method is to run <tt>.detach()</tt> on a tensor to prevent future computations from being tracked. This can be handy when cloning a tensor.</p>

<h2>Building a Simple Neural Network</h2>

<p>Note that the rest of the article will need some knowledge of Machine Learning or Neural Networks.<br />
We will discover other features like Dataloaders, Criterions and Optimizers using an example on the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">IRIS Dataset</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'iris.csv'</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>

<p>PyTorch has 2 really useful libraries for Neral Networks:<br /></p>
<ol>
  <li><tt>torch.nn</tt> generally imported as <tt>nn</tt> <br /></li>
  <li><tt>torch.nn.functional</tt> generally imported as <tt>F</tt></li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'target'</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'target'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># One Hot Encoding
</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>  
<span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<p>To convert the training tensors into a dataset and to make things like batch gradient descent easier, we may use the <tt>TensorDataset</tt> and <tt>DataLoader</tt> classes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trainset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<p>We will create batches of size 30. We will shuffle the training data so that the batches are not biased. However this is not necessary for the test data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trainloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">testloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p>To access the batches:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 4 batches. Each batch is two-dimensional, one for the features(X) and other for the classes(y).
</span><span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">trainloader</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2
2
2
2
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># To access the index, X data and Y data
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 tensor([[5.7000, 2.9000, 4.2000, 1.3000],
        [7.4000, 2.8000, 6.1000, 1.9000],
        [6.9000, 3.1000, 5.1000, 2.3000],
        [5.3000, 3.7000, 1.5000, 0.2000],
        [5.4000, 3.0000, 4.5000, 1.5000],
        [5.5000, 4.2000, 1.4000, 0.2000],
        [6.1000, 2.6000, 5.6000, 1.4000],
        [5.5000, 2.6000, 4.4000, 1.2000],
        [6.0000, 3.4000, 4.5000, 1.6000],
        [5.7000, 2.5000, 5.0000, 2.0000],
        [4.4000, 3.2000, 1.3000, 0.2000],
        [5.0000, 3.6000, 1.4000, 0.2000],
        [4.9000, 3.1000, 1.5000, 0.1000],
        [7.0000, 3.2000, 4.7000, 1.4000],
        [6.3000, 2.5000, 4.9000, 1.5000],
        [6.1000, 2.8000, 4.0000, 1.3000],
        [6.5000, 3.0000, 5.8000, 2.2000],
        [5.1000, 3.8000, 1.5000, 0.3000],
        [5.0000, 3.0000, 1.6000, 0.2000],
        [4.6000, 3.6000, 1.0000, 0.2000],
        [5.7000, 2.8000, 4.5000, 1.3000],
        [4.9000, 3.1000, 1.5000, 0.1000],
        [5.8000, 2.7000, 4.1000, 1.0000],
        [5.1000, 3.3000, 1.7000, 0.5000],
        [4.4000, 2.9000, 1.4000, 0.2000],
        [6.2000, 2.2000, 4.5000, 1.5000],
        [7.9000, 3.8000, 6.4000, 2.0000],
        [7.7000, 3.8000, 6.7000, 2.2000],
        [6.3000, 2.9000, 5.6000, 1.8000],
        [5.5000, 2.5000, 4.0000, 1.3000]]) tensor([1, 2, 2, 0, 1, 0, 2, 1, 1, 2, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 2, 2, 2, 1])

1 tensor([[5.0000, 3.2000, 1.2000, 0.2000],
        [4.9000, 3.1000, 1.5000, 0.1000],
        [6.7000, 3.3000, 5.7000, 2.1000],
        [5.5000, 3.5000, 1.3000, 0.2000],
        [5.4000, 3.9000, 1.7000, 0.4000],
        [6.0000, 3.0000, 4.8000, 1.8000],
        [5.1000, 2.5000, 3.0000, 1.1000],
        [5.9000, 3.0000, 4.2000, 1.5000],
        [7.6000, 3.0000, 6.6000, 2.1000],
        [6.4000, 2.7000, 5.3000, 1.9000],
        [5.1000, 3.8000, 1.6000, 0.2000],
        [6.9000, 3.1000, 5.4000, 2.1000],
        [7.2000, 3.6000, 6.1000, 2.5000],
        [5.1000, 3.5000, 1.4000, 0.2000],
        [6.5000, 3.2000, 5.1000, 2.0000],
        [5.5000, 2.4000, 3.7000, 1.0000],
        [5.6000, 2.8000, 4.9000, 2.0000],
        [6.3000, 3.4000, 5.6000, 2.4000],
        [7.3000, 2.9000, 6.3000, 1.8000],
        [5.9000, 3.2000, 4.8000, 1.8000],
        [6.8000, 2.8000, 4.8000, 1.4000],
        [4.9000, 2.5000, 4.5000, 1.7000],
        [5.1000, 3.5000, 1.4000, 0.3000],
        [6.2000, 3.4000, 5.4000, 2.3000],
        [5.7000, 2.8000, 4.1000, 1.3000],
        [6.1000, 3.0000, 4.9000, 1.8000],
        [5.5000, 2.4000, 3.8000, 1.1000],
        [5.7000, 2.6000, 3.5000, 1.0000],
        [5.0000, 3.5000, 1.6000, 0.6000],
        [5.6000, 2.7000, 4.2000, 1.3000]]) tensor([0, 0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 2, 2, 0, 2, 1, 2, 2, 2, 1, 1, 2, 0, 2,
        1, 2, 1, 1, 0, 1])

2 tensor([[5.0000, 3.3000, 1.4000, 0.2000],
        [5.8000, 2.6000, 4.0000, 1.2000],
        [5.6000, 3.0000, 4.1000, 1.3000],
        [5.0000, 2.0000, 3.5000, 1.0000],
        [6.4000, 2.9000, 4.3000, 1.3000],
        [5.1000, 3.8000, 1.9000, 0.4000],
        [5.6000, 2.9000, 3.6000, 1.3000],
        [6.7000, 3.1000, 4.4000, 1.4000],
        [6.1000, 3.0000, 4.6000, 1.4000],
        [4.5000, 2.3000, 1.3000, 0.3000],
        [6.7000, 3.1000, 5.6000, 2.4000],
        [5.7000, 3.8000, 1.7000, 0.3000],
        [4.8000, 3.1000, 1.6000, 0.2000],
        [6.5000, 2.8000, 4.6000, 1.5000],
        [6.0000, 2.2000, 5.0000, 1.5000],
        [6.5000, 3.0000, 5.2000, 2.0000],
        [6.3000, 3.3000, 6.0000, 2.5000],
        [4.9000, 2.4000, 3.3000, 1.0000],
        [7.1000, 3.0000, 5.9000, 2.1000],
        [4.4000, 3.0000, 1.3000, 0.2000],
        [6.4000, 3.2000, 4.5000, 1.5000],
        [5.0000, 2.3000, 3.3000, 1.0000],
        [6.7000, 3.1000, 4.7000, 1.5000],
        [5.4000, 3.7000, 1.5000, 0.2000],
        [6.3000, 3.3000, 4.7000, 1.6000],
        [5.1000, 3.7000, 1.5000, 0.4000],
        [6.1000, 2.9000, 4.7000, 1.4000],
        [5.2000, 2.7000, 3.9000, 1.4000],
        [5.1000, 3.4000, 1.5000, 0.2000],
        [4.8000, 3.4000, 1.9000, 0.2000]]) tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 0, 0, 1, 2, 2, 2, 1, 2, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0])

3 tensor([[4.3000, 3.0000, 1.1000, 0.1000],
        [6.4000, 3.1000, 5.5000, 1.8000],
        [6.9000, 3.1000, 4.9000, 1.5000],
        [5.6000, 3.0000, 4.5000, 1.5000],
        [6.0000, 2.9000, 4.5000, 1.5000],
        [7.2000, 3.0000, 5.8000, 1.6000],
        [6.6000, 2.9000, 4.6000, 1.3000],
        [5.8000, 2.7000, 5.1000, 1.9000],
        [5.0000, 3.4000, 1.5000, 0.2000],
        [6.3000, 2.8000, 5.1000, 1.5000],
        [6.2000, 2.8000, 4.8000, 1.8000],
        [4.7000, 3.2000, 1.3000, 0.2000],
        [5.7000, 3.0000, 4.2000, 1.2000],
        [4.6000, 3.1000, 1.5000, 0.2000],
        [4.6000, 3.2000, 1.4000, 0.2000],
        [6.7000, 2.5000, 5.8000, 1.8000],
        [5.8000, 2.7000, 3.9000, 1.2000],
        [4.6000, 3.4000, 1.4000, 0.3000],
        [6.3000, 2.3000, 4.4000, 1.3000],
        [6.0000, 2.7000, 5.1000, 1.6000],
        [5.2000, 3.5000, 1.5000, 0.2000],
        [5.5000, 2.3000, 4.0000, 1.3000],
        [5.8000, 2.8000, 5.1000, 2.4000],
        [5.8000, 4.0000, 1.2000, 0.2000],
        [4.8000, 3.0000, 1.4000, 0.1000],
        [6.4000, 2.8000, 5.6000, 2.2000],
        [6.8000, 3.2000, 5.9000, 2.3000],
        [5.8000, 2.7000, 5.1000, 1.9000],
        [6.7000, 3.3000, 5.7000, 2.5000],
        [5.4000, 3.9000, 1.3000, 0.4000]]) tensor([0, 2, 1, 1, 1, 2, 1, 2, 0, 2, 2, 0, 1, 0, 0, 2, 1, 0, 1, 1, 0, 1, 2, 0,
        0, 2, 2, 2, 2, 0])
</code></pre></div></div>

<p><strong>Creating the Model Class</strong><br />
To define a Neural Network, we need to define a class which inherits from the <tt>nn.Module</tt> class. Here is where we can define all the layers, activation functions, embeddings etc. Here we will create a simple model with 2 hidden layers:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">h1</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">h2</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span><span class="n">h1</span><span class="p">)</span>    <span class="c1"># input layer
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">)</span>            <span class="c1"># hidden layer
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h2</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>  <span class="c1"># output layer
</span>        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Define the activation functions for the layers
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Instantiate the Model class using parameter defaults:
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Defining the loss function and optimizer:</strong><br />
The loss function is conventionally defined as <strong>criterion</strong>. THe various loss functions are available in <tt>torch.nn</tt> library and the optimizers are available in the <tt>torch.optim</tt> library. Here we will use Cross Entropy Loss and the Adam Optimizer</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="c1"># lr: Learning Rate
</span></code></pre></div></div>

<p><strong>Training the model</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">i</span><span class="o">+=</span><span class="mi">1</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
        
        
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    
    
    <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="mi">10</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'epoch: {i:2}  loss: {loss.item():10.8f}'</span><span class="p">)</span>


</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>epoch:  1  loss: 1.09409952
epoch: 11  loss: 0.96279073
epoch: 21  loss: 0.87627226
epoch: 31  loss: 0.72676194
epoch: 41  loss: 0.66376936
epoch: 51  loss: 0.68377405
epoch: 61  loss: 0.68355846
epoch: 71  loss: 0.64891487
epoch: 81  loss: 0.66468638
epoch: 91  loss: 0.66344351
</code></pre></div></div>

<p>Above, since the backward() function accumulates gradients, to avoid mixing up of gradients between minibatches, you have to zero them out beore backpropagating on the next batch. <tt>optimizer.zero_grad()</tt> is used for this. <tt>optimizer.step</tt>  performs a parameter update based on the current gradient (stored in <tt>.grad</tt> attribute of a parameter) and the update rule</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="o">*</span><span class="mi">4</span><span class="p">),</span> <span class="n">losses</span><span class="p">)</span> <span class="c1">#epochs*4 because each epoch is made of 4 batches and for each of them a loss is calculated
</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'epoch'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/blog/assets/img/intro-to-pytorch/graph.png" alt="graph" /></p>

<p><strong>Saving the Model’s Parameters</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s">'IrisDatasetModel.pt'</span><span class="p">)</span>
</code></pre></div></div>

<p>Only the parameters of the model are saved and not the model itself. 
For more information on saving and loading visit https://pytorch.org/tutorials/beginner/saving_loading_models.html</p>

<p><strong>Loading a Model</strong></p>

<p>We’ll load a new model object and test it as we had before to make sure it worked.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">new_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'IrisDatasetModel.pt'</span><span class="p">))</span>
<span class="n">new_model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model(
  (fc1): Linear(in_features=4, out_features=10, bias=True)
  (fc2): Linear(in_features=10, out_features=10, bias=True)
  (out): Linear(in_features=10, out_features=3, bias=True)
)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">y_val</span> <span class="o">=</span> <span class="n">new_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'{loss:.8f}'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.69291312
</code></pre></div></div>

<p>References:</p>
<ol>
  <li>https://pytorch.org/docs/stable/index.html</li>
  <li>https://www.udacity.com/course/deep-learning-pytorch–ud188</li>
  <li>https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch</li>
  <li>https://en.wikipedia.org/wiki/PyTorch</li>
  <li>https://www.analyticsvidhya.com/blog/2018/02/pytorch-tutorial/</li>
  <li>https://www.analyticsvidhya.com/blog/2019/09/introduction-to-pytorch-from-scratch/</li>
</ol>

<p><a href="https://colab.research.google.com/drive/1odpZAS42UzVk16TGol_ta24DtyX1ZB2B"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></p>

<hr />

                </article>
                <div class="sidebar">
    <h1>Related Articles</h1>
    <div class="temp-div">
        <span id="category">Compsoc/Diode/Piston</span>
        <span id="title">An Introduction to PyTorch</span>
    </div>
    <div class="related-articles"></div>
</div>

            </div>

            <section class="author" itemprop="author">
    <div class="details" itemscope itemtype="http://schema.org/Person">
        <img itemprop="image" class="img-rounded" src="/blog/assets/img/authors/Shruthan.jpg" alt="Shruthan R">
        <p class="def">Author</p>
        <h3 class="name">
            <a itemprop="name" href="/">Shruthan R</a>
        </h3>
        <p class="desc"></p>
        <a itemprop="email" class="email" href="mailto:shruthan@outlook.com">shruthan@outlook.com</a>
    </div>
</section>

            <section class="comments">

	<div id="fb-root"></div>
	
	<script>(function(d, s, id) {
		var js, fjs = d.getElementsByTagName(s)[0];
		if (d.getElementById(id)) return;
		js = d.createElement(s); js.id = id;
		js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.5&appId=978128892233940";
		fjs.parentNode.insertBefore(js, fjs);
		}(document, 'script', 'facebook-jssdk'));
	</script>
</section>
            
            <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20http://ieee.nitk.ac.in/blog/intro-to-pytorch/%20via%20&#64;IEEE_NITK&hashtags=Deep Learning,Machine Learning,Python,"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://ieee.nitk.ac.in//blog//intro-to-pytorch/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
    <a aria-label="Share on Google Plus" href="https://plus.google.com/share?url=http://ieee.nitk.ac.in//blog//intro-to-pytorch/"
    onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google+">
        <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg>
    </a>
</section>

            <footer>
    <p>Made with <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> and <span class="love">❤</span> by <a href="https://ieeenitk.org">IEEE NITK</a></p>
</footer>
<script src="/blog/assets/js/main.js"></script>
            
<script>
	$(document).ready(function(){
		if($("pre code").length){
			
			$.getScript('../assets/js/highlight.pack.js', function(){
    			// load the highlighter script only if we have code snippets.
    			$('pre code').each(function(i, block) {
    				hljs.highlightBlock(block);
  				});	
    		});
		}
	});
</script>
        </section>

    </body>
</html>
            
