<!DOCTYPE html>
<html lang="pt-br">
        <head>
        <!-- Give a small description in 3-4 sentences about the website -->
        <meta name="abstract" content="IEEE NITK Student Chapter under Mangalore SubSection and Bangalore Section was founded in 1988. Currently it has three SIGs namely Computer Society, Circuits and Systems, Signal Processing Society and Piston. It also has two Affinty Groups namely Special Interest Group in Humanitarian Technologies (SIGHT) and Women in Engineering (WiE)">
        <!-- Important Keywords to be noted in the website  -->
        <meta name="keywords" content="IEEE, IEEE NITK, NITK Surathkal, NITK, Clubs in NITK, Technical Clubs in NITK">
        <!-- Tell the spider to index the first page and other pages as well-->
        <meta name="robots" content="index, follow">    
        <!-- How often should spiders come back to your page -->
        <meta name="revisit-after" content="3 days">

        <!-- Copyright regarding the website -->
        <meta name="copyright" content="IEEE NITK">
        <!-- Tells Google Bot not to duplicate description -->
        <meta name="googlebot" content="noodp">
        <!-- Language for the website -->
        <meta name="language" content="English">  

        <!-- Web Author for the image -->
        <meta name="web_author" content="IEEE NITK">
        <meta name="author" content="Salman Shah">
        <!-- Email ID -->
        <meta name="contact" content="ieee@nitk.edu.in" />
        <!-- Email ID to reply to -->
        <meta name="reply-to" content="ieee@nitk.edu.in">

        <!-- Refers to distribution of the page -->
        <meta name="distribution" content="global">
        <!-- Generator or formatter tag -->
        <meta name="generator" content="Jekyll">
        <!-- Disallow spammers for the webpage -->
        <meta name="no-email-collection" content="http://www.metatags.info/nospamharvesting">
        <!-- Rating for the page -->
        <meta name="rating" content="general">
        <!-- Content Type for the page -->
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <!-- Fixing viewport on mobile views -->
        <meta name="viewport" content="width=device-width; initial-scale=1; maximum-scale=1.0">

        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">

        <meta name="description" content="Where we explore the funny world of recent language models">

        <!-- Google Authorship Markup -->
        <!-- Social: Twitter -->
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:site" content="@IEEE_NITK">
        <meta name="twitter:title" content="Generate Strange Text with GPT-2">
        <meta name="twitter:description" content="Where we explore the funny world of recent language models">
        <meta property="twitter:image:src" content="http://ieee.nitk.ac.in/blog/assets/img/blog-image.png">

        <!-- Social: Facebook / Open Graph -->
        <meta property="fb:app_id" content="0011038251882641" />
        <meta property="og:url" content="http://ieee.nitk.ac.in/blog/generate-text-with-gpt2/" />
        <meta property="og:title" content="Generate Strange Text with GPT-2">
        <meta property="og:image" content="http://ieee.nitk.ac.in/blog/assets/img/blog-image.png">
        <meta property="og:description" content="Where we explore the funny world of recent language models">
        <meta property="og:site_name" content="A blog about our findings and musings">

        <!-- Social: Google+ / Schema.org  -->
        <meta itemprop="name" content="Generate Strange Text with GPT-2"/>
        <meta itemprop="description" content="Where we explore the funny world of recent language models">
        <meta itemprop="image" content="http://ieee.nitk.ac.in/blog/assets/img/blog-image.png"/>
        <meta name="msapplication-TileColor" content="#ffffff">
        <meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
        <meta name="theme-color" content="#ffffff">

        <!-- Windows 8 Tile Icons -->
        <meta name="application-name" content="IEEE-NITK Blog">
        <meta name="msapplication-TileColor" content="#0562DC">
        <meta name="msapplication-square70x70logo" content="smalltile.png" />
        <meta name="msapplication-square150x150logo" content="mediumtile.png" />
        <meta name="msapplication-wide310x150logo" content="widetile.png" />
        <meta name="msapplication-square310x310logo" content="largetile.png" />
        
        <!-- Android Lolipop Theme Color -->
        <meta name="theme-color" content="#0562DC">

        <link rel="author" href="https://plus.google.com/?rel=author">

        <!-- Favicon -->
        <link rel="shortcut icon" href="/blog/assets/img/icons/favicon.ico" type="image/x-icon" />
        <link rel="manifest" href="/blog/assets/img/icons/manifest.json">
        <!-- Include Font Awesome -->
        <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

        <title>Generate Strange Text with GPT-2</title>

        <!-- highlighter theme -->
        <link rel="stylesheet" href="/blog/assets/css/monokai-sublime.css">

        <link rel="stylesheet" href="/blog/assets/css/main.css">
        <link rel="canonical" href="/blog/generate-text-with-gpt2/">
        <link rel="alternate" type="application/rss+xml" title="A blog about our findings and musings"
        href="http://ieee.nitk.ac.in/blog/feed.xml" />

        

        <!-- jQuery for highlighter -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-106590424-3"></script>
        <script src="/blog/assets/js/sidebar.js"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-106590424-3');
            $(document).ready(function(){
                loadArticles();
            });
        </script>

    </head>

    <body>
        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-email" viewBox="0 0 1024 1024"><path class="path1" d="M950.857 859.429v-438.857q-18.286 20.571-39.429 37.714-153.143 117.714-243.429 193.143-29.143 24.571-47.429 38.286t-49.429 27.714-58.571 14h-1.143q-27.429 0-58.571-14t-49.429-27.714-47.429-38.286q-90.286-75.429-243.429-193.143-21.143-17.143-39.429-37.714v438.857q0 7.429 5.429 12.857t12.857 5.429h841.143q7.429 0 12.857-5.429t5.429-12.857zM950.857 258.857v-14t-0.286-7.429-1.714-7.143-3.143-5.143-5.143-4.286-8-1.429h-841.143q-7.429 0-12.857 5.429t-5.429 12.857q0 96 84 162.286 110.286 86.857 229.143 181.143 3.429 2.857 20 16.857t26.286 21.429 25.429 18 28.857 15.714 24.571 5.143h1.143q11.429 0 24.571-5.143t28.857-15.714 25.429-18 26.286-21.429 20-16.857q118.857-94.286 229.143-181.143 30.857-24.571 57.429-66t26.571-75.143zM1024 237.714v621.714q0 37.714-26.857 64.571t-64.571 26.857h-841.143q-37.714 0-64.571-26.857t-26.857-64.571v-621.714q0-37.714 26.857-64.571t64.571-26.857h841.143q37.714 0 64.571 26.857t26.857 64.571z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-rss" viewBox="0 0 805 1024"><path class="path1" d="M219.429 768q0 45.714-32 77.714t-77.714 32-77.714-32-32-77.714 32-77.714 77.714-32 77.714 32 32 77.714zM512 838.286q1.143 16-9.714 27.429-10.286 12-26.857 12h-77.143q-14.286 0-24.571-9.429t-11.429-23.714q-12.571-130.857-105.429-223.714t-223.714-105.429q-14.286-1.143-23.714-11.429t-9.429-24.571v-77.143q0-16.571 12-26.857 9.714-9.714 24.571-9.714h2.857q91.429 7.429 174.857 46t148 103.714q65.143 64.571 103.714 148t46 174.857zM804.571 839.429q1.143 15.429-10.286 26.857-10.286 11.429-26.286 11.429h-81.714q-14.857 0-25.429-10t-11.143-24.286q-6.857-122.857-57.714-233.429t-132.286-192-192-132.286-233.429-58.286q-14.286-0.571-24.286-11.143t-10-24.857v-81.714q0-16 11.429-26.286 10.286-10.286 25.143-10.286h1.714q149.714 7.429 286.571 68.571t243.143 168q106.857 106.286 168 243.143t68.571 286.571z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol><symbol id="icon-angle-down" viewBox="0 0 658 1024"><path class="path1" d="M614.286 420.571q0 7.429-5.714 13.143l-266.286 266.286q-5.714 5.714-13.143 5.714t-13.143-5.714l-266.286-266.286q-5.714-5.714-5.714-13.143t5.714-13.143l28.571-28.571q5.714-5.714 13.143-5.714t13.143 5.714l224.571 224.571 224.571-224.571q5.714-5.714 13.143-5.714t13.143 5.714l28.571 28.571q5.714 5.714 5.714 13.143z"/></symbol><symbol id="icon-github-alt" viewBox="0 0 951 1024"><path class="path1" d="M365.714 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM731.429 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM822.857 694.857q0-68.571-39.429-116.571t-106.857-48q-23.429 0-111.429 12-40.571 6.286-89.714 6.286t-89.714-6.286q-86.857-12-111.429-12-67.429 0-106.857 48t-39.429 116.571q0 50.286 18.286 87.714t46.286 58.857 69.714 34.286 80 16.857 85.143 4h96q46.857 0 85.143-4t80-16.857 69.714-34.286 46.286-58.857 18.286-87.714zM950.857 594.286q0 118.286-34.857 189.143-21.714 44-60.286 76t-80.571 49.143-97.143 27.143-98 12.571-95.429 2.571q-44.571 0-81.143-1.714t-84.286-7.143-87.143-17.143-78.286-29.429-69.143-46.286-49.143-65.714q-35.429-70.286-35.429-189.143 0-135.429 77.714-226.286-15.429-46.857-15.429-97.143 0-66.286 29.143-124.571 61.714 0 108.571 22.571t108 70.571q84-20 176.571-20 84.571 0 160 18.286 60-46.857 106.857-69.143t108-22.286q29.143 58.286 29.143 124.571 0 49.714-15.429 96 77.714 91.429 77.714 227.429z"/></symbol></defs></svg>

        <header class="header-post" role="banner">
    <div class="content">
        
            <time itemprop="datePublished" datetime="2019-10-02 07:06:00 +0530" class="date">02 Oct 2019</time>
        
        <h1 class="post-title" itemprop="name">Generate Strange Text with GPT-2</h1>
        <p itemprop="description" class="subtitle">Where we explore the funny world of recent language models</p>
    </div>
     <a class="down" data-scroll href="#scroll"><svg class="icon icon-angle-down"><use xlink:href="#icon-angle-down"></use></svg></a>
     <div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search...">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>

<div id="fade" class="overlay"></div>
<a id="slide" class="slideButton fade">
    <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    <svg id="close" class="icon-menu"><use xlink:href="#icon-close"></use></svg>
</a>
<aside id="sidebar">
<nav id="navigation">
  <h2>MENU</h2>
  <ul>
    
    
      <li><a href="http://ieee.nitk.ac.in/blog">Home</a></li>
    
    
    
      <li><a href="http://ieee.nitk.ac.in/blog/tags">Tags</a></li>
    
    
    
      <li><a href="http://ieee.nitk.ac.in/">Main Website</a></li>
    
    
    
      <li><a href="http://ieee.nitk.ac.in/gyan">Gyan</a></li>
    
    
    <li><a class="feed" href="http://ieee.nitk.ac.in/blog/feed.xml" title="Atom/RSS feed">Feed</a></li>
  </ul>
</nav>
</aside>
<a id="search" class="dosearch">
    <svg class="icon-menu icon-search"><use xlink:href="#icon-search"></use></svg>
</a>

</header>

        <section class="post" itemscope itemtype="http://schema.org/BlogPosting">

            <div class="content-box">
                <article role="article" id="scroll" class="post-content" itemprop="articleBody">
                    <p>GPT-2 refers to a set of <em>Language Models</em> released recently by OpenAI. My objective is to find weird ways that GPT-2 can fail, and also some smart things it is capable of. I’ll be using <a href="https://talktotransformer.com/">talktotransformer</a> for this task. It’s a convenient tool that asks for a short text prompt, and then generates several lines of text to continue the prompt (no coding required!).</p>

<p>First, let’s see if you’re paying attention. Check out the following two samples:</p>

<blockquote>
  <p><strong>Department of applied mechanics and hydraulics of National Institute of Technology - Karnataka (NIT-K), Suratkal will host a 3-day workshop on coastal reservoirs as a sustainable strategy for water security on its Srinivasnagar premises from July 22. T G Seetharam, director, IIT Guwahati, will be the chief guest and inaugurate the seminar. K Uma Maheshwar Rao, director, NITK will preside over the inaugural function.</strong><br /><br />
The seminar will offer a comprehensive overview of the coastal water management sector including hydrometeorology, hydrodynamic, hydrometeorology and hydrography.<br /><br />
It will be addressed by experts in the various fields including engineering, hydraulics, hydrology, hydrography and marine ecology/oceanography, besides coastal water managers and engineers.<br /><br />
Seetharam said while it was an emerging sector with great potential, no clear strategies were being pursued for conservation and recovery of flood prone areas.</p>
</blockquote>

<p>Makes sense? What about this one:</p>

<blockquote>
  <p><strong>Department of applied mechanics and hydraulics of National Institute of Technology - Karnataka (NIT-K), Suratkal will host a 3-day workshop on coastal reservoirs as a sustainable strategy for water security on its Srinivasnagar premises from July 22. T G Seetharam, director, IIT Guwahati, will be the chief guest and inaugurate the seminar. K Uma Maheshwar Rao, director, NITK will preside over the inaugural function.</strong><br /><br />
“We will address the problem of coastal development in a holistic way and in many ways, the sea cannot be separated from the land. We have to look at the whole development area and take the sea into account,” M S Rathore, head of the Centre for Research in Development and Environment (CRED), said.<br /><br />
This is the second such seminar under CRED. The first seminar was held on August 6.</p>
</blockquote>

<p>Which one do you think is real, and which one was generated?</p>

<p>I’m dead serious: which one do you think is fake?</p>

<p>You can re-read them carefully to make your choice.</p>

<p><br /><br /><br /><br /><br /><br />
Okay ready?
<br /><br /><br /><br /><br /><br /></p>

<p>Hah, Trick question! Turns out, <em>both</em> were generated by GPT-2. The real article is <a href="https://timesofindia.indiatimes.com/home/education/news/three-day-workshop-on-coastal-reservoir-at-nitk-concludes-today/articleshow/70356325.cms">this one from the Times Of India</a></p>

<p>I took the first paragraph from that article (<strong>bold</strong> in the generated samples), fed it into the transformer, and after a few trials, out came the above two examples.</p>

<p>Notice, in the second fake sample, the thing was talking about a certain <em>Centre for Research in Development and Environment</em>, which does sound like a typical government-style name for an organization. Also, it gave an <em>acronym</em> as well, and the initials match up! <em>CRED</em> isn’t even a real organization as far as I know, and the model somehow birthed it out of thin air.</p>

<p>Now the obvious question is - can it generate poetry?</p>

<p>Let’s see what the transformer spews out when prompted with Dylan Thomas’ famous piece:</p>

<blockquote>
  <p><strong>Do not go gentle into that good night,</strong><br />
<strong>Old age should burn and rave at close of day;</strong><br />
<strong>Rage, rage against the dying of the light.</strong>
<br /><br />
When the night is past and the moon is down,<br />
I shall sit and watch it, till I have heard the dawn,<br />
Which I shall know to be a new life.
<br /><br />
Old age ought not to be fathomed,<br />
Forthwith shall we know it to be a night;<br />
Rage is, and a good song shall be sung.
<br /><br />
And in the hour of my death I shall not sleep;<br />
I shall sing and tell of the night long;<br />
I shall hear your song and say: How sweet the evening</p>
</blockquote>

<p>Much deep, so poetry.</p>

<p>Somehow the model knows how poems like these are structured, where the lines should break, and what kind of deep and wise sounding words a romantic poet writes.</p>

<p>And now for some pure mischief:</p>

<blockquote>
  <p><strong>Kejriwal announces: “I’m a vampire. I’m actually from outer space”</strong><br />
-On his alleged visit in 2013, to the Indian Space Research Organisation (ISRO) to observe the launch of a satellite of its foreign rivals<br />
-Claims to have created the world’s second most powerful economy with “nearly double” of the GDP.<br />
But that’s not all. Mr Kejriwal, who is the chief minister of Delhi, claims to have been abducted last year by the security forces, tortured and forced to drink demon urine.<br />
And he has also said that he is a vampire, which is a type of supernatural creature that feeds on blood.<br />
He said: “So that’s how I was born. It’s not my fault. I got stolen by the forces during a traffic raid. It took my arm off.”<br />
Mr Kejriwal, the former Delhi chief minister, says he is a “vampire” because he believes God has cursed him.<br />
A video obtained by Daily News India of Mr Kejriwal during his first public address was shown in front of the National Conference’s main square in Delhi.<br /></p>
</blockquote>

<p>Among other things:</p>

<ol>
  <li>It knows what a vampire does (<em>supernatural creature that feeds on blood.</em>)</li>
  <li>It associates Kejriwal with the position of CM of Delhi, which is great! But it first says he is the CM, then says he’s the former CM. So it has problems with consistency.</li>
  <li>The article seems to match the general flow of a news piece, albeit confused by the mention of vampires.</li>
</ol>

<p>How exactly does this thing work?</p>

<hr />

<h3 id="birds-eye-view-of-gpt-2">Birds eye view of GPT-2</h3>

<p>GPT-2 stands for Generative Pretrained Transformer 2. Generative because it is, well, generating text. Pretrained because the guys who made it trained it on a HUGE corpus of outgoing links from Reddit. Pretraining is useful because this let’s us fine-tune the model to our particular use case with a lot fewer examples. Transformer because that’s what the building blocks of this thing are called.</p>

<p><img src="/blog_src/assets/img/GPT2_gen/decoder.png" alt="Stack of Decoders" />
<em><a href="https://jalammar.github.io/illustrated-bert/"><center>Source</center></a></em></p>

<p>GPT-2 is a <strong>Language Model</strong>, which is a fancy way of saying that, when given a prompt, it gives a distribution of which word should come next.</p>

<p>(It’s not exactly words that it gives probabilities for, more like pieces of words. For simplicity, let’s just assume that it has a vocabulary of words among which it decides.)</p>

<p>But neural networks deal with numbers, not strings. So we convert each word into a fixed-length vector of numbers, so that the input sentences are converted into an array of vectors (all of the same length). This is what’s called an <strong>Embedding</strong>. GPT-2 also considers the context in which each word is used, and gives a slightly different set of numbers for different contexts.</p>

<p>As an example of why context matters, notice that the word ‘break’ has different meanings in ‘break the glass’ vs ‘summer break’.</p>

<p>So now have converted the input string into a list of vectors. Now we feed it into the <strong>Decoder</strong> block, which essentially looks like this::</p>

<p><img src="/blog_src/assets/img/GPT2_gen/transformer_block.svg" alt="Transformer Block" />
<em><a href="http://www.peterbloem.nl/blog/transformers"><center>Source</center></a></em></p>

<p>Attention is an operation which converts an input sequence to an output sequence of the <em>same length</em>. The output is also a list of vectors of fixed length, just like the input sequence.</p>

<p>MLP is just your garden variety vanilla feedforward net, although it can be a convolutional block as well.</p>

<p>Layer norm is a method of speeding up the training (by allowing gradients to flow more easily), and adding regularization to the network.</p>

<p>Also, we use skip connections (those arrows that go around blocks) because this let’s us directly propagate information from earlier layers to later layers.</p>

<p>(For more in-depth and hands-on explanations, check out the resources at the end.)</p>

<hr />

<h3 id="closing-thoughts">Closing thoughts</h3>

<p>You can make GPT-2 do all kinds of fun stuff: generate Lord of the Rings fanfiction, brew up some recipes, fake popular science news and generate some sweet, sweet political propaganda.</p>

<p>When it works, it’s language use can be wicked good at fooling people who are just skimming (including me, of course). It can hold onto a topic over several sentences, and flow of words is also quite natural on the surface.</p>

<p>It’s when you read beyond just the words themselves and try to grasp the big picture, it feels more like it took a big pile of what makes up the internet, mixed the sentences around thoroughly, and spit out a barely coherent soup.</p>

<p>So does this thing have <em>actual</em> language understanding?</p>

<hr />

<h3 id="resources">Resources</h3>

<p>If you wanna play around with GPT-2 yourself:</p>
<ul>
  <li><a href="https://talktotransformer.com/">Talk To Transformer</a></li>
  <li><a href="https://transformer.huggingface.co/">Write With Transformer</a>
<br /><br /></li>
</ul>

<p>If you want to dive deep and get your hands dirty with real code, here are some resources:</p>
<ul>
  <li><a href="http://www.peterbloem.nl/blog/transformers">Transformers from Scratch</a>: Really intuitive explanations of how the transformer works. Shows how to implement one in PyTorch.</li>
  <li><a href="https://jalammar.github.io/illustrated-bert/">The illustrated BERT, ELMo and co.</a>: A great overview of the various developments that led to the awesomeness of recent NLP models.</li>
  <li><a href="https://huggingface.co/transformers/index.html">HuggingFace’s transformers library</a>
<br /><br /></li>
</ul>

<p>For more fun stuff that Neural Nets do:</p>
<ul>
  <li><a href="https://aiweirdness.com/">AI Weirdness</a></li>
  <li><a href="https://srconstantin.wordpress.com/2019/02/25/humans-who-are-not-concentrating-are-not-general-intelligences/">Humans Who Are Not Concentrating Are Not General Intelligences</a></li>
  <li><a href="https://slatestarcodex.com/2019/02/19/gpt-2-as-step-toward-general-intelligence/">GPT-2 as a step toward General Intelligence</a></li>
</ul>

<!-- Docs to Markdown version 1.0β17 -->

                </article>
                <div class="sidebar">
    <h1>Related Articles</h1>
    <div class="temp-div">
        <span id="category">Compsoc</span>
        <span id="title">Generate Strange Text with GPT-2</span>
    </div>
    <div class="related-articles"></div>
</div>

            </div>

            <section class="author" itemprop="author">
    <div class="details" itemscope itemtype="http://schema.org/Person">
        <img itemprop="image" class="img-rounded" src="/blog/assets/img/authors/kinshuk.jpg" alt="Kinshuk Kashyap">
        <p class="def">Author</p>
        <h3 class="name">
            <a itemprop="name" href="/">Kinshuk Kashyap</a>
        </h3>
        <p class="desc">Randomized statistical pattern matcher</p>
        <a itemprop="email" class="email" href="mailto:kinshuk.other@gmail.com">kinshuk.other@gmail.com</a>
    </div>
</section>

            <section class="comments">

	<div id="fb-root"></div>
	
	<script>(function(d, s, id) {
		var js, fjs = d.getElementsByTagName(s)[0];
		if (d.getElementById(id)) return;
		js = d.createElement(s); js.id = id;
		js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.5&appId=978128892233940";
		fjs.parentNode.insertBefore(js, fjs);
		}(document, 'script', 'facebook-jssdk'));
	</script>
</section>
            
            <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20http://ieee.nitk.ac.in/blog/generate-text-with-gpt2/%20via%20&#64;IEEE_NITK&hashtags=IEEE NITK,CompSoc,"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://ieee.nitk.ac.in//blog//generate-text-with-gpt2/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
    <a aria-label="Share on Google Plus" href="https://plus.google.com/share?url=http://ieee.nitk.ac.in//blog//generate-text-with-gpt2/"
    onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google+">
        <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg>
    </a>
</section>

            <footer>
    <p>Made with <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> and <span class="love">❤</span> by <a href="https://ieeenitk.org">IEEE NITK</a></p>
</footer>
<script src="/blog/assets/js/main.js"></script>
            
<script>
	$(document).ready(function(){
		if($("pre code").length){
			
			$.getScript('../assets/js/highlight.pack.js', function(){
    			// load the highlighter script only if we have code snippets.
    			$('pre code').each(function(i, block) {
    				hljs.highlightBlock(block);
  				});	
    		});
		}
	});
</script>
        </section>

    </body>
</html>
            
