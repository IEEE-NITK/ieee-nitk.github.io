<!DOCTYPE html>
<html lang="pt-br">
        <head>
        <!-- Give a small description in 3-4 sentences about the website -->
        <meta name="abstract" content="IEEE NITK Student Chapter under Mangalore SubSection and Bangalore Section was founded in 1988. Currently it has three SIGs namely Computer Society, Circuits and Systems, Signal Processing Society and Piston. It also has two Affinty Groups namely Special Interest Group in Humanitarian Technologies (SIGHT) and Women in Engineering (WiE)">
        <!-- Important Keywords to be noted in the website  -->
        <meta name="keywords" content="IEEE, IEEE NITK, NITK Surathkal, NITK, Clubs in NITK, Technical Clubs in NITK">
        <!-- Tell the spider to index the first page and other pages as well-->
        <meta name="robots" content="index, follow">    
        <!-- How often should spiders come back to your page -->
        <meta name="revisit-after" content="3 days">

        <!-- Copyright regarding the website -->
        <meta name="copyright" content="IEEE NITK">
        <!-- Tells Google Bot not to duplicate description -->
        <meta name="googlebot" content="noodp">
        <!-- Language for the website -->
        <meta name="language" content="English">  

        <!-- Web Author for the image -->
        <meta name="web_author" content="IEEE NITK">
        <meta name="author" content="Salman Shah">
        <!-- Email ID -->
        <meta name="contact" content="ieee@nitk.edu.in" />
        <!-- Email ID to reply to -->
        <meta name="reply-to" content="ieee@nitk.edu.in">

        <!-- Refers to distribution of the page -->
        <meta name="distribution" content="global">
        <!-- Generator or formatter tag -->
        <meta name="generator" content="Jekyll">
        <!-- Disallow spammers for the webpage -->
        <meta name="no-email-collection" content="http://www.metatags.info/nospamharvesting">
        <!-- Rating for the page -->
        <meta name="rating" content="general">
        <!-- Content Type for the page -->
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <!-- Fixing viewport on mobile views -->
        <meta name="viewport" content="width=device-width; initial-scale=1; maximum-scale=1.0">

        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">

        <meta name="description" content="An introduction to a commonly used Recurrent Neural Network">

        <!-- Google Authorship Markup -->
        <!-- Social: Twitter -->
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:site" content="@IEEE_NITK">
        <meta name="twitter:title" content="Long Short Term Memory Neural Networks">
        <meta name="twitter:description" content="An introduction to a commonly used Recurrent Neural Network">
        <meta property="twitter:image:src" content="https://ieee.nitk.ac.in/blog/assets/img/blog-image.png">

        <!-- Social: Facebook / Open Graph -->
        <meta property="fb:app_id" content="0011038251882641" />
        <meta property="og:url" content="https://ieee.nitk.ac.in/blog/Long-Short-Term-Neural-Networks/" />
        <meta property="og:title" content="Long Short Term Memory Neural Networks">
        <meta property="og:image" content="https://ieee.nitk.ac.in/blog/assets/img/blog-image.png">
        <meta property="og:description" content="An introduction to a commonly used Recurrent Neural Network">
        <meta property="og:site_name" content="A blog about our findings and musings">

        <!-- Social: Google+ / Schema.org  -->
        <meta itemprop="name" content="Long Short Term Memory Neural Networks"/>
        <meta itemprop="description" content="An introduction to a commonly used Recurrent Neural Network">
        <meta itemprop="image" content="https://ieee.nitk.ac.in/blog/assets/img/blog-image.png"/>
        <meta name="msapplication-TileColor" content="#ffffff">
        <meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
        <meta name="theme-color" content="#ffffff">

        <!-- Windows 8 Tile Icons -->
        <meta name="application-name" content="IEEE-NITK Blog">
        <meta name="msapplication-TileColor" content="#0562DC">
        <meta name="msapplication-square70x70logo" content="smalltile.png" />
        <meta name="msapplication-square150x150logo" content="mediumtile.png" />
        <meta name="msapplication-wide310x150logo" content="widetile.png" />
        <meta name="msapplication-square310x310logo" content="largetile.png" />
        
        <!-- Android Lolipop Theme Color -->
        <meta name="theme-color" content="#0562DC">

        <link rel="author" href="https://plus.google.com/?rel=author">

        <!-- Favicon -->
        <link rel="shortcut icon" href="/blog/assets/img/icons/favicon.ico" type="image/x-icon" />
        <link rel="manifest" href="/blog/assets/img/icons/manifest.json">
        <!-- Include Font Awesome -->
        <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

        <title>Long Short Term Memory Neural Networks</title>

        <!-- highlighter theme -->
        <link rel="stylesheet" href="/blog/assets/css/monokai-sublime.css">

        <link rel="stylesheet" href="/blog/assets/css/main.css">
        <link rel="canonical" href="/blog/Long-Short-Term-Neural-Networks/">
        <link rel="alternate" type="application/rss+xml" title="A blog about our findings and musings"
        href="https://ieee.nitk.ac.in/blog/feed.xml" />

        

        <!-- jQuery for highlighter -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-106590424-3"></script>
        <script src="/blog/assets/js/sidebar.js"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-106590424-3');
            $(document).ready(function(){
                loadArticles();
            });
        </script>

    </head>

    <body>
        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-email" viewBox="0 0 1024 1024"><path class="path1" d="M950.857 859.429v-438.857q-18.286 20.571-39.429 37.714-153.143 117.714-243.429 193.143-29.143 24.571-47.429 38.286t-49.429 27.714-58.571 14h-1.143q-27.429 0-58.571-14t-49.429-27.714-47.429-38.286q-90.286-75.429-243.429-193.143-21.143-17.143-39.429-37.714v438.857q0 7.429 5.429 12.857t12.857 5.429h841.143q7.429 0 12.857-5.429t5.429-12.857zM950.857 258.857v-14t-0.286-7.429-1.714-7.143-3.143-5.143-5.143-4.286-8-1.429h-841.143q-7.429 0-12.857 5.429t-5.429 12.857q0 96 84 162.286 110.286 86.857 229.143 181.143 3.429 2.857 20 16.857t26.286 21.429 25.429 18 28.857 15.714 24.571 5.143h1.143q11.429 0 24.571-5.143t28.857-15.714 25.429-18 26.286-21.429 20-16.857q118.857-94.286 229.143-181.143 30.857-24.571 57.429-66t26.571-75.143zM1024 237.714v621.714q0 37.714-26.857 64.571t-64.571 26.857h-841.143q-37.714 0-64.571-26.857t-26.857-64.571v-621.714q0-37.714 26.857-64.571t64.571-26.857h841.143q37.714 0 64.571 26.857t26.857 64.571z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-rss" viewBox="0 0 805 1024"><path class="path1" d="M219.429 768q0 45.714-32 77.714t-77.714 32-77.714-32-32-77.714 32-77.714 77.714-32 77.714 32 32 77.714zM512 838.286q1.143 16-9.714 27.429-10.286 12-26.857 12h-77.143q-14.286 0-24.571-9.429t-11.429-23.714q-12.571-130.857-105.429-223.714t-223.714-105.429q-14.286-1.143-23.714-11.429t-9.429-24.571v-77.143q0-16.571 12-26.857 9.714-9.714 24.571-9.714h2.857q91.429 7.429 174.857 46t148 103.714q65.143 64.571 103.714 148t46 174.857zM804.571 839.429q1.143 15.429-10.286 26.857-10.286 11.429-26.286 11.429h-81.714q-14.857 0-25.429-10t-11.143-24.286q-6.857-122.857-57.714-233.429t-132.286-192-192-132.286-233.429-58.286q-14.286-0.571-24.286-11.143t-10-24.857v-81.714q0-16 11.429-26.286 10.286-10.286 25.143-10.286h1.714q149.714 7.429 286.571 68.571t243.143 168q106.857 106.286 168 243.143t68.571 286.571z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol><symbol id="icon-angle-down" viewBox="0 0 658 1024"><path class="path1" d="M614.286 420.571q0 7.429-5.714 13.143l-266.286 266.286q-5.714 5.714-13.143 5.714t-13.143-5.714l-266.286-266.286q-5.714-5.714-5.714-13.143t5.714-13.143l28.571-28.571q5.714-5.714 13.143-5.714t13.143 5.714l224.571 224.571 224.571-224.571q5.714-5.714 13.143-5.714t13.143 5.714l28.571 28.571q5.714 5.714 5.714 13.143z"/></symbol><symbol id="icon-github-alt" viewBox="0 0 951 1024"><path class="path1" d="M365.714 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM731.429 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM822.857 694.857q0-68.571-39.429-116.571t-106.857-48q-23.429 0-111.429 12-40.571 6.286-89.714 6.286t-89.714-6.286q-86.857-12-111.429-12-67.429 0-106.857 48t-39.429 116.571q0 50.286 18.286 87.714t46.286 58.857 69.714 34.286 80 16.857 85.143 4h96q46.857 0 85.143-4t80-16.857 69.714-34.286 46.286-58.857 18.286-87.714zM950.857 594.286q0 118.286-34.857 189.143-21.714 44-60.286 76t-80.571 49.143-97.143 27.143-98 12.571-95.429 2.571q-44.571 0-81.143-1.714t-84.286-7.143-87.143-17.143-78.286-29.429-69.143-46.286-49.143-65.714q-35.429-70.286-35.429-189.143 0-135.429 77.714-226.286-15.429-46.857-15.429-97.143 0-66.286 29.143-124.571 61.714 0 108.571 22.571t108 70.571q84-20 176.571-20 84.571 0 160 18.286 60-46.857 106.857-69.143t108-22.286q29.143 58.286 29.143 124.571 0 49.714-15.429 96 77.714 91.429 77.714 227.429z"/></symbol></defs></svg>

        <header class="header-post" role="banner">
    <div class="content">
        
            <time itemprop="datePublished" datetime="2019-08-22 16:30:30 +0530" class="date">22 Aug 2019</time>
        
        <h1 class="post-title" itemprop="name">Long Short Term Memory Neural Networks</h1>
        <p itemprop="description" class="subtitle">An introduction to a commonly used Recurrent Neural Network</p>
    </div>
     <a class="down" data-scroll href="#scroll"><svg class="icon icon-angle-down"><use xlink:href="#icon-angle-down"></use></svg></a>
     <div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search...">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>

<div id="fade" class="overlay"></div>
<a id="slide" class="slideButton fade">
    <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    <svg id="close" class="icon-menu"><use xlink:href="#icon-close"></use></svg>
</a>
<aside id="sidebar">
<nav id="navigation">
  <h2>MENU</h2>
  <ul>
    
    
      <li><a href="https://ieee.nitk.ac.in/blog">Home</a></li>
    
    
    
      <li><a href="https://ieee.nitk.ac.in/blog/tags">Tags</a></li>
    
    
    
      <li><a href="https://ieee.nitk.ac.in/">Main Website</a></li>
    
    
    
      <li><a href="https://ieee.nitk.ac.in/gyan">Gyan</a></li>
    
    
    <li><a class="feed" href="https://ieee.nitk.ac.in/blog/feed.xml" title="Atom/RSS feed">Feed</a></li>
  </ul>
</nav>
</aside>
<a id="search" class="dosearch">
    <svg class="icon-menu icon-search"><use xlink:href="#icon-search"></use></svg>
</a>

</header>

        <section class="post" itemscope itemtype="http://schema.org/BlogPosting">

            <div class="content-box">
                <article role="article" id="scroll" class="post-content" itemprop="articleBody">
                    <p>Long Short Term Memory Neural Networks or LSTM Neural Network is a commonly used Recurrent Neural Network model that is most commonly used in tasks like speech recognition, music generation etc. Lets take a deep dive into what Recurrent Neural Networks are and why we need and use LSTMs.</p>

<h1 id="long-short-term-memory-neural-networks">Long Short Term Memory Neural Networks</h1>

<h3 id="recurrent-neural-networks-and-why-lstms">Recurrent Neural Networks and Why LSTMs?</h3>

<p><strong>Recurrent Neural Networks</strong> are those Neural Networks that we use to process information that require us to keep informed of previous information. In simpler words, when we want a model to perform a certain category of tasks like speech recognition, music generation, machine translation, sentiment classification, etc, all of which invloves keeping track of previous information (like keeping track of all the words while processing a sentence during machine translation), a normal Neural Network cannot do this, while a Recurrent Neural Network, which keeps a track of the past information in it’s internal state addresses this issue.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Recurrent Neural Network - Unrolled</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/blog/assets/img/Long-Short-Term-Memory-Neural-Networks/RNN.png" alt="RNN" height="50%" width="50%" /></td>
    </tr>
  </tbody>
</table>

<p>The above diagram is that of a basic Recurrent Neural Network, the chained structure is appropriate for modeling sequences and using this structure for Neural Networks have proven to work for the above mentioned applications. LSTMs come into play when a certain problem occurs within RNNs itself.</p>

<h4 id="problem-of-vanishing-gradients-due-to-long-term-dependencies"><strong>Problem of Vanishing Gradients due to Long Term Dependencies</strong></h4>

<p>Consider the following small sentence: <code class="highlighter-rouge">She was riding her cycle</code> . Predicting the word ‘her’ using RNNs is relatively easy since it just has to process the immediate words next to it, but predicting some sort of information that requires some sort of context from earlier, for example, mentioning <code class="highlighter-rouge">I am from France</code> in the beginning of a paragraph, but having to predict the language that was spoken much later in the text. Here, as the gap between relevant information grows, it becomes difficult for RNNs to connect both information to make a valid prediction. This happens because while calculating gradients for the weights during backpropagation, the values from one end of the sequence may find it difficult to influence that in other ends of the sequence which may or may not play an important role in prediction. This is the case in normal RNNs, where ‘long range dependencies’ are not really supported.</p>

<p>This is where LSTMs come into play!</p>

<h2 id="long-short-term-memory-networks">Long Short Term Memory Networks</h2>

<p><strong>Long Short Term Memory Networks</strong> (usually just called <code class="highlighter-rouge">LSTMs</code>) are a <em>special kind</em> of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp; Schmidhuber (1997). They are explicitly designed to avoid the long-term dependency problem by remembering information for long periods of time, and this is possible by introducing ‘memory cells’ which keep track of these dependencies throughout the sequence.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Long Short Term Memory Network</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/blog/assets/img/Long-Short-Term-Memory-Neural-Networks/LSTM.png" alt="LSTM" height="50%" width="50%" /></td>
    </tr>
  </tbody>
</table>

<p>This is how an LSTM looks like, it follows the same chain like structure like that of the RNNs, but it contains several added gates. It may seem difficult to process this as a whole, but we’ll walk through this step by step.</p>

<p><strong>Note</strong>: The sigmoid function returns a value between 0 to 1 and this case, values very close to either 0 or 1 and hence is commonly used in our gates to make a particular decision.</p>

<p>The following are states and gates involved in an LSTM cell:</p>
<ol>
  <li><strong>Activation layer</strong>: This layer consists of the activation values like the normal RNNs do</li>
  <li><strong>Memory Cell</strong> or <strong>Candidate layer</strong>: This layer is involved in keeping track of dependencies</li>
  <li><strong>Update Gate</strong>: Sigmoid function that decides whether or not the memory cell should keep track of the dependecy</li>
  <li><strong>Forget Gate</strong>: Sigmoid function that decides whether or not the memory cell should leave or forget the dependency</li>
  <li><strong>Output Gate</strong>: Sigmoid function that helps us filter what parts of the memory cell layer we want to pass into the output</li>
</ol>

<table>
  <thead>
    <tr>
      <th style="text-align: center">LSTM Structure Inside</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/blog/assets/img/Long-Short-Term-Memory-Neural-Networks/LSTM2.png" alt="LSTM2" /></td>
    </tr>
  </tbody>
</table>

<p>If the diagram is overwhelming, the following equations may help you to walk through the process.</p>

<script type="math/tex; mode=display">% <![CDATA[
\hat{c}^{<t>} = tanh(  W_{c}[ a^{<t-1>},x^{<t>} ] + b_c  ) %]]></script>

<p>This is the calculation for a memory cell initially which takes into account the previous activation layer and input layers’ weights, and adds it to a bias, while passing the resultant to a tanh function that returns a score between -1 and 1, which in turn carries a dependency.</p>

<script type="math/tex; mode=display">% <![CDATA[
\Gamma_{u} = \sigma(  W_{u}[ a^{<t-1>},x^{<t>} ] + b_u  ) %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\Gamma_{f} = \sigma(  W_{f}[ a^{<t-1>},x^{<t>} ] + b_f  ) %]]></script>

<p>Here, the decision is made whether or not to keep track of the dependency with the help of the update and forget gates, which are sigmoid layers.</p>

<script type="math/tex; mode=display">% <![CDATA[
\Gamma_{o} = \sigma(  W_{o}[ a^{<t-1>},x^{<t>} ] + b_o  ) %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
c^{<t>} = \Gamma_{u}*\hat{c}^{<t>} + \Gamma_{f}*c^{<t-1>} %]]></script>

<p>Here, we decide what exactly to update into the memory cell, which either retains the dependency from earlier or updates it to a new value based on the decision made by the update and forget gates. Hence, the output will be filtered. This is done by running a sigmoid function layer to decide which parts of the memory cell we will send to the output and while the memory cells will be passed through tanh and then passed through the output gate to get only the filtered output. Here, our memory cells are updated appropriately.</p>

<script type="math/tex; mode=display">% <![CDATA[
a^{<t>} = \Gamma_{o}*tanh(c^{<t>}) %]]></script>

<p>The activation layer is influenced by certain memory cell values decided upon by the output gate, and is appropriately updated and passed onto the next cell.</p>

<p>The resultant <script type="math/tex">% <![CDATA[
\hat{y}^{<t>} %]]></script> vector is obtained by passing the activation layer through a softmax function, but do note that this step is dependent on what problem we’re solving and isn’t part of the general LSTM framework.</p>

<h2 id="character-to-character-lstm-model">Character to Character LSTM Model</h2>

<p>We are going to use a two layer LSTM model with 512 hidden nodes in each layer. We will make the model read a text file that contains text from a transcript, in this example we will make the model read an exerpt from ‘The Outcasts’, we will then use the same sequence but shifted by one character as a target.</p>

<p>Before we get started, here are some key terms to get used to:</p>

<ol>
  <li><strong>Vocabulary</strong>: This is a set of every character that our model requires</li>
  <li><strong>LSTM Cell</strong> : We will make use of pyTorch’s LSTM cell that has the structure, as explained earlier</li>
  <li><strong>Hidden State or Activation State</strong>: This is a vector of size(batch_size, hidden_size), the bigger dimension of the hidden_size, the more robust our model becomes but at the expense of computational cost. This vector acts as our short-term memory and is updated by the input at the time step t.</li>
  <li><strong>Layers of an LSTM</strong>: We can stack LSTM cells on top of each other to obtain a layered LSTM model. This is done by passing the output of the first LSTM cell from the input to the second LSTM cell at any given time t, this gives a deeper network.</li>
</ol>

<p><strong>The code with explanation in comments is provided at the references section of this article</strong></p>

<p>The model receives an “A” initially as an input to the LSTM cell at time t=0. After that, we get the output with the size of our vocabulary from the memory cell. If we apply softmax function to the output, we get the probabilities of the characters. Then we take ‘k’ most probable characters and then sample one character according to their probability in the space of these ‘k’ characters. This sampled character is now going to be input to the LSTM cell at time t=1, and so on.</p>

<p>Always remember that pytorch expects batch dimensions everywhere, and don’t forget to convert numpy arrays into torch tensors and back to numpy again since we are dealing with integers in the end and we need them to look up actual characters.</p>

<p>Here is some of the output while monitoring the losses during training:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch: 0, Batch: 0, Train Loss: 4.375697, Validation Loss: 4.338589
Epoch: 0, Batch: 5, Train Loss: 3.400858, Validation Loss: 3.402019
Epoch: 1, Batch: 0, Train Loss: 3.239244, Validation Loss: 3.299909
Epoch: 1, Batch: 5, Train Loss: 3.206378, Validation Loss: 3.262871
.
.
.
Epoch: 49, Batch: 0, Train Loss: 1.680400, Validation Loss: 2.052764
Epoch: 49, Batch: 5, Train Loss: 1.701830, Validation Loss: 2.061397
</code></pre></div></div>

<p>Here is what the model learnt and generated in the first epoch:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">First Epoch</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/blog/assets/img/Long-Short-Term-Memory-Neural-Networks/sampleoutput1.png" alt="epoch1" /></td>
    </tr>
  </tbody>
</table>

<p>and this is the outcome after 50 epochs:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Fiftieth Epoch</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="/blog/assets/img/Long-Short-Term-Memory-Neural-Networks/sampleoutput2.png" alt="epoch50" /></td>
    </tr>
  </tbody>
</table>

<h3 id="final-outcome">Final Outcome</h3>

<p>We can see that the resulting sample that at the 50th epoch doesn’t make much sense, but it does show signs that the model has learned a lot, like some words, some sentence structure and syntax. Now all we need to do is to tweak the model’s hyper-parameters to make it better, and we will have a better character to character model than we started off with.</p>

<h2 id="references--">References -</h2>

<ul>
  <li><a href="https://gist.github.com/nimbus98/9c23ef7825d1c2ce42058d09f780ce08">Code for Character-to-Character Model</a></li>
</ul>

                </article>
                <div class="sidebar">
    <h1>Related Articles</h1>
    <div class="temp-div">
        <span id="category">Compsoc</span>
        <span id="title">Long Short Term Memory Neural Networks</span>
    </div>
    <div class="related-articles"></div>
</div>

            </div>

            <section class="author" itemprop="author">
    <div class="details" itemscope itemtype="http://schema.org/Person">
        <img itemprop="image" class="img-rounded" src="/blog/assets/img/authors/akashnair.jpeg" alt="Akash Nair">
        <p class="def">Author</p>
        <h3 class="name">
            <a itemprop="name" href="/">Akash Nair</a>
        </h3>
        <p class="desc">I like Food</p>
        <a itemprop="email" class="email" href="mailto:akash30121998@gmail.com">akash30121998@gmail.com</a>
    </div>
</section>

            <section class="comments">

	<div id="fb-root"></div>
	
	<script>(function(d, s, id) {
		var js, fjs = d.getElementsByTagName(s)[0];
		if (d.getElementById(id)) return;
		js = d.createElement(s); js.id = id;
		js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.5&appId=978128892233940";
		fjs.parentNode.insertBefore(js, fjs);
		}(document, 'script', 'facebook-jssdk'));
	</script>
</section>
            
            <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20https://ieee.nitk.ac.in/blog/Long-Short-Term-Neural-Networks/%20via%20&#64;IEEE_NITK&hashtags=Recurrent Neural Networks,Long Short Term Memory Neural Networks,Machine Learning,"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://ieee.nitk.ac.in//blog//Long-Short-Term-Neural-Networks/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
    <a aria-label="Share on Google Plus" href="https://plus.google.com/share?url=https://ieee.nitk.ac.in//blog//Long-Short-Term-Neural-Networks/"
    onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;" title="Share on Google+">
        <svg class="icon icon-google-plus"><use xlink:href="#icon-google-plus"></use></svg>
    </a>
</section>

            <footer>
    <p>Made with <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> and <span class="love">❤</span> by <a href="https://ieeenitk.org">IEEE NITK</a></p>
</footer>
<script src="/blog/assets/js/main.js"></script>
            
<script>
	$(document).ready(function(){
		if($("pre code").length){
			
			$.getScript('../assets/js/highlight.pack.js', function(){
    			// load the highlighter script only if we have code snippets.
    			$('pre code').each(function(i, block) {
    				hljs.highlightBlock(block);
  				});	
    		});
		}
	});
</script>
        </section>

    </body>
</html>
            
