---
layout: post
title: "How NASA discovered planets using Artificial Intelligence"
author_github:  gurupunskill
date: 2017-12-21 16:44:03
image: '/assets/img/'
description: 'How deep learning was used to discover an exoplanet'
tags:
- Space
- Machine Learning
- Deep Learning

categories:
- CompSoc
github_username: 'gurupunskill'
comments: true
---

A Convolutional Neural Network.  
**A Convolutional Neural Network and 90 minutes of training**  

Clearly, the hard part wasn't the training or the model but to get the _training set_.  
But what is the _data_? and where did we get it form?  

>On December 14, 2017 NASA [announced](https://www.nasa.gov/press-release/artificial-intelligence-nasa-data-used-to-discover-eighth-planet-circling-distant-star) the discovery of an eigth planet in the *Kepler-90* system. *Kepler-90* is a Sun-like star 2545 light-years from Earth. The discovery was made using Artificial Intelligence.

### Where did the data come from?  

NASA has this huge camera up in space that has been sending us videos of _stars of interest_ every 29.4 minutes. That's an oversimplification of what the [Kepler Space Telescope](https://www.nasa.gov/mission_pages/kepler/main/index.html "Kepler Mission Homepage") does, but it's enough for the purpose and essence of this article. _Kepler_'s 94.6 megapixel<sup>[1]</sup> camera has a fixed field of view, which means it's been looking at the same spot in the sky since it launched in 2009.  

That's a _lot_ of data.  

### What _is_ the data?
To understand the data, we need to first understand how NASA detect planets.   Planets don't emit light. So how does _Kepler_ even see planets that are lightyears away?   Planets don't emit light... but Stars do. _Kepler_ measures the brightness of stars over several years and since planets orbit around stars, they must cross the line of sight between kepler and the star periodically.  

![Kepler Transit Graph](/blog/assets/img/space-ml/kepler-graph.gif "Kepler Transit Graph")

So, _Kepler_ puts up a nice _brightness vs time_ graph and sends it back to us every 29 minutes. A _periodic dip_ in the graph implies that there must be a planet. NASA classified periodic dips that may be consistent with transiting planets as **Threshold Crossing Events** (TCE). The _Kepler_ team initially heterogenously put together TCEs by hand, but later on they designed an _Auto Vetter_ that automatically eliminates uninteresting candidates.  

And this gives the raw unprocessed dataset: The _Autovetter Planet Candidate Catalogue_ hosted at the _[NASA Exoplanet Archive](http://exoplanetarchive.ipac.caltech.edu/)_. The light curves that were used to find the new planets are from the _[Mikulski Archive for Space Telescopes](http://archive.stsci.edu/)_. Each light curve in this dataset consists of integrated flux (brightness) measurements spaced at 29.4 minute intervals for upto 4 years<sup>[2]</sup>.  

### Processing the dataset

The researchers at NASA then followed this up with _flattening_ the light curve by fitting a [basis spline](https://en.wikipedia.org/wiki/B-spline) (removes the low frequency variability) and then dividing it by the [best-fit spline](https://en.wikipedia.org/wiki/Curve_fitting) (connect the dots and smoothen them).  

They had to then _fold_ (to make it event centered<sup>[3]</sup>) and _bin_ the flattened curve to produce a 1D vector.  

Binning and folding are two methods of making trends in messy data more apparent.  

>Folding is useful when a source has periodic variability; the data is plotted in terms of phase, such that all the data are plotted together as a single period, in order to see what the repeated pattern of variability is. 

Folding is basically splitting the graph into blocks and then overlapping all the blocks on top of one another. This way we can notice repeated patterns more easily.  

> Random variations from this pattern can be reduced by binning the data in time, which involves splitting the phase range into steps (bins) in which all the data are averaged, using a weighted mean.

Even if we do fold the graph, it's still really messy since there may be random variations in the data. Binning can be thought as averaging the data out at a folded point in time to give 1 point.  

Original Light Curve |  Folded Curve      |   Binned Curve	 |
:-------------------:|:------------------:|:--------------------:|
![Light Curve](/blog/assets/img/space-ml/lightcurve.png) | ![Folded Curve](/blog/assets/img/space-ml/foldedcurve.png) | ![Binned Curve](/blog/assets/img/space-ml/binnedcurve.png)|

The problem with binning the TCE dataset is that all the periods in our light curves are of different lengths. Different planets have different orbital lengths, different orbital periods and different distances from Earth. Hence, different periods of transits. So, how _did_ they bin these light curves to act as inputs?

They generated two separate _views_ of the same light curve:

   1. A _global view_ which was generated by binning by a fraction of the TCE **period**. These views, the _global views_ of the light curves, were all binned to the same length, and each view represents the same number of points, on average, across all light curves. 
   
   2. A _local view_ which was generated by binning by a fraction of the TCE **duration**. This results in a zoomed in view of the TCEs. It treats short and long TCEs equally and so might miss out on some information. It looks at only a part of the curve.

[![Source](/blog/assets/img/space-ml/binning_example.png)](https://www.cfa.harvard.edu/~avanderb/kepler90i.pdf)

### The dataset is now ready.
The researchers then passed these views as inputs into the their multilayered convolutional network and trained.  

### Resources

Although this post is not going to divulge into the details of the neural network, you can check it out on their research paper: [Identifying Exoplanets with Deep learning](https://www.cfa.harvard.edu/~avanderb/kepler90i.pdf).  

If you want to know more about binning, folding and how to apply them with numpy, go [here](https://www.southampton.ac.uk/~sdc1g08/BinningFolding.html).  

<sup>[1] Kepler only sends back about 5% of the 94.6MP based on our _Stars of Interest_</sup>  
<sup>[2] Roughly 70000 points </sup>  
<sup>[3] There are 4 years of data and planets don't cross all the time. So, we need to remove the parts where they aren't crossing.</sup>  
