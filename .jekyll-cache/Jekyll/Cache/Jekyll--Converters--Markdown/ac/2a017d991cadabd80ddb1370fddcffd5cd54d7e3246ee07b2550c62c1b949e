I"Q<p>In this article, we will use a Reinforcement Learning based method to solve the Switch Riddle problem. So, let’s start with defining the riddle.</p>

<h2 id="switch-riddle">Switch Riddle</h2>

<p>There are n prisoners in prison and a warden. The Warden decides to free the prisoners if they can solve the following problem. So, every day the Warden will select one of the prisoners randomly and send him to an interrogation room which consists of a light bulb with a switch. If the prisoner in the room can tell that all other prisoners including him have been to the room at least once then the Warden will free all of them otherwise kill all of them.
Except for the prisoner in the room, other prisoners are unaware of the fact that who got selected on that particular day to go to the interrogation room.</p>

<p><img src="/blog/assets/img/Solving-Switch-Riddle-using-RL/SwitchRiddle.png" alt="Switch Riddle" title="Switch Riddle" /></p>

<p><a href="https://github.com/iassael/learning-to-communicate">Image Source</a></p>

<p>Now, the prisoner in the interrogation room can switch on or off the bulb to send some indication to the next prisoner. He can also tell the warden that everyone has been to the room at least once or decide not to say anything. If his claim is correct, then all are set free otherwise they are all killed.</p>

<h2 id="problem-formulation">Problem Formulation</h2>

<p>We can observe that switch riddle problem comprises of multiple agents working towards completing a single task. Therefore, the puzzle fits in a Multi-Agent setup where the agents are collaborating to complete the task.
We will use a Deep Reinforcement learning based algorithms called the <strong>DIAL( Differential Inter Agent Learning)</strong> to solve this riddle.</p>

<p>The things that we need to define for most RL problems are states, actions, and rewards. So let’s formulate these:</p>

<h4 id="state">State</h4>
<p>The state for each agent or prisoner is whether the agent is in the interrogation room or not, i.e., if the prisoner n is in the room, then he receives a 1 and others 0 for that particular day or time step.</p>

<h4 id="action">Action</h4>
<p>There are two choices the prisoners have</p>

<p>1 - Tell the Warden.</p>

<p>2 - Don’t tell the Warden.</p>

<h4 id="reward">Reward</h4>
<p>The reward is received at the end of the episode.</p>

<p>+1 - Completing the task.</p>

<p>-1 - Telling the Warden before everyone has gone to the room once.</p>

<p>0 - If the prisoners can’t conclude within the max number of time steps.</p>

<p>In the implementation, the maximum number of timesteps is set to <script type="math/tex">4 \times n - 6</script> where n is the number of agents.
In addition to state, action, and reward, since the agents need to communicate through the light bulb to send some information to the next prisoner, we also have this one-bit communication channel, i.e., the bulb.</p>

<h4 id="messages">Messages</h4>
<p>Training Time - In DIAL during training, the messages are continuous; i.e., it can take values ranging from 0 to 1.</p>

<p>Test Time - While testing the agents can only communicate with 0 or 1.</p>

<p>0 -&gt; Bulb is off</p>

<p>1 -&gt; Bulb is On</p>

<h2 id="dial-algorithm">DIAL Algorithm</h2>

<p>The following diagram shows the architecture of the model used in DIAL.</p>

<p><img src="/blog/assets/img/Solving-Switch-Riddle-using-RL/DIAL.png" alt="DIAL" title="DIAL" /></p>

<p><a href="https://papers.nips.cc/paper/6042-learning-to-communicate-with-deep-multi-agent-reinforcement-learning.pdf">Image Source</a></p>

<p>There are two possible ways in which we can train our model -</p>

<p>1) Without Parameter sharing</p>

<p>2) With Parameter sharing</p>

<p>Parameter Sharing means all the agents share the same neural network parameters or we can say that there is a single central network that learns for all the agents.</p>

<p>While in No-Parameter sharing there are separate networks for each agent.</p>

<h3 id="implementation-details--">Implementation details -</h3>

<p>Following is the model architecture used in the <a href="https://papers.nips.cc/paper/6042-learning-to-communicate-with-deep-multi-agent-reinforcement-learning.pdf">original paper</a> -</p>

<p><img src="/blog/assets/img/Solving-Switch-Riddle-using-RL/DIALarchitecture.png" alt="DIAL Architecture" title="DIALarchitecture" /></p>

<p><a href="https://papers.nips.cc/paper/6042-learning-to-communicate-with-deep-multi-agent-reinforcement-learning.pdf">Image Source</a></p>

<p>1.Instead of directly giving input observation as 0 or 1 to the neural net, it is passed through an embedding layer.</p>

<p>2.The message from the previous agent is passed through a 1-layer MLP before giving it as an input to the RNN.</p>

<p>3.The previous actions of the agent and agent id are passed through lookup tables.</p>

<p>4.The final state embedding is given by -</p>

<script type="math/tex; mode=display">z^a_t = (TaskMLP(o^a_t) + MLP[|M| , 128 ](m_{t-1}) + Lookup(u^a_{t-1}) + Lookup(a))</script>

<p>5.For better stability and performance a batch normalization layer can be used to preprocess <script type="math/tex">m_{t-1}</script>.</p>

<p>6.<script type="math/tex">z^a_t</script> is processed through a 2-layer RNN with GRUs, <script type="math/tex">h^a_{1,t} =GRU[128, 128] (z^a_t, h^a_{1,t−1})</script>, which is used to approximate the agent’s action-observation history.</p>

<p>7.
Finally, the output <script type="math/tex">h^a_{2,t}</script> of the top GRU layer, is passed through a 2-layer MLP <script type="math/tex">Q^a_t, m^a_t =MLP[128, 128,(|U| + |M|)](h^a_{2,t})</script>.</p>

<h2 id="references--">References -</h2>

<ul>
  <li><a href="https://github.com/iassael/learning-to-communicate">Original Lua Implementation</a></li>
  <li><a href="https://colab.research.google.com/gist/MJ10/2c0d1972f3dd1edcc3cd17c636aac8d2/dial.ipynb">Pytorch implementation of DIAL -By Moksh Jain</a></li>
</ul>

:ET