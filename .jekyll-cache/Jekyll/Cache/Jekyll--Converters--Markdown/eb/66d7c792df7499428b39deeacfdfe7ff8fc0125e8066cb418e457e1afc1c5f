I"d%<p><a href="https://colab.research.google.com/drive/1odpZAS42UzVk16TGol_ta24DtyX1ZB2B"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></p>
<h2 id="an-introduction-to-pytorch">An Introduction to PyTorch</h2>

<p>PyTorch was released in early 2017 by Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan and has been making a pretty big impact in the deep learning community. It's is a Python-based scientific computing package targeted to be a: (1) A replacement for NumPy to use the power of GPUs and (2) A deep learning research platform that provides maximum flexibility and speed. It's developed as an open source project by the Facebook AI Research team, and is being adopted by teams everywhere in industry and academia and is very comfortable to learn and use. It is based on the Torch library and has both a Python and C++ frontend(though the Python frontend is more 'polished'). PyTorch is also very pythonic, meaning, it feels more natural to use it if you already are a Python developer. </p>

<h2>What we'll see</h2>
<p><br />
In this post, we will first look at the basics of Tensors(which are the building blocks of anything you do using PyTorch), and operations on them. We will then have a look at gradients and how they are computed in PyTorch. Finally we shall build a simple Neural Network for the IRIS dataset using PyTorch</p>

<h2>Prerequisites</h2>
<p>Knowledge of Python(3.x) is required. Knowledge of NumPy will be useful but is not necessary. For the last part(of building a Neural Network), a basic understanding of a simple neural network is assumed</p>

<h2>Installation</h2>
<p>Details of installation may be found <a href="https://pytorch.org/get-started/locally/">here</a>. However, to start off with, I would recommend using <a href="https://pytorch.org/get-started/locally/">Google Colab</a> or <a href="https://azure.microsoft.com/en-us/develop/pytorch/">Microsoft Azure</a></p>

<p>We'll first have a look at the building blocks of PyTorch (most other deep learning libraries also) which are Tensors.</p>

<h2>Tensors</h2>

<p>All computations in PyTorch generally consist of operations on Tensors. Tensors can be thought of as a generalisation of vectors or matrices in 1 or more dimensions, or more simply, are like arrays in a programming language like C. In some cases tensors are used as a replacement for NumPy to use the power of GPUs A 1D Tensor is like a 1D array, 2D Tensor like a 2D array and so on. Let’s see how we can use them.</p>

<h4>Importing torch</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'1.1.0'
</code></pre></div></div>

<p>Tensors are available in the torch library as <a href="https://pytorch.org/docs/stable/tensors.html"><strong><tt>torch.Tensor</tt></strong></a>. It is like a multidimensional array which can have elements of a single datatype. Computations between tensors are allowed only if the tensors share the same data type(dtype).</p>

<p>First lets create a tensor <strong>from a python list</strong>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">myTensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">myTensor</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1, 2, 3])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">myTensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">myTensor1</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[1., 2., 3.],
        [4., 5., 6.]])
</code></pre></div></div>

<p>Above, we see that the function determines the data type based on the inputs. We may use different constructors to specify the data type we need as:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fltTensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">fltTensor</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1., 2., 3.]) &lt;br&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">intTensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">IntTensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">intTensor</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1, 2, 3], dtype=torch.int32) &lt;br&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">longTensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">longTensor</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1, 2, 3])
</code></pre></div></div>

<p>We may also achieve the above using the dtype attribute in torch.tensor:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">intTensor1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">int</span><span class="p">)</span>
<span class="n">intTensor1</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1, 2, 3], dtype=torch.int32)
</code></pre></div></div>

<p><strong>Note:</strong> There is a subtle difference between the functions <tt>torch.tensor</tt> and <tt>torch.Tensor</tt>. torch.Tensor is an alias to torch.FloatTensor whereas torch.tensor determines the data type based on the input.</p>

<p>For more information on datatypes, check <a href="https://pytorch.org/docs/stable/tensors.html">here</a>.</p>

<h4>To and From a NumPy array</h4>

<p>Converting a torch tensor to a numpy array and vice versa is very easy and hence makes it easy to access other libraries like Scikit-Learn and Matplotlib</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
</code></pre></div></div>
<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">arr</span><span class="p">))</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1 2 3 4 5]
int32
&lt;class 'numpy.ndarray'&gt;
</code></pre></div></div>

<p>We can use <tt>torch.from_numpy</tt> or <tt>torch.as_tensor</tt>:</p>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="n">x</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1, 2, 3, 4, 5], dtype=torch.int32)
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="nb">type</span><span class="p">())</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;class 'torch.Tensor'&gt;
torch.IntTensor
</code></pre></div></div>

<p>Note that we can also use torch.tensor for this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="n">x1</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([1, 2, 3, 4, 5], dtype=torch.int32)
</code></pre></div></div>

<p>The difference between <tt>torch.tensor</tt> and <tt>torch.from_numpy</tt>(or <tt>torch.as_tensor</tt>) is that when we use the former, a copy of the original tensor is made and stored in <tt>x1</tt>(above). Any changes made to <tt>x1</tt> will not affect <tt>arr</tt>(the numpy array) and vice versa. However, when we use the latter function, the tensor created (x in the example above) points to the same location in memory as does <tt>arr</tt>. Hence any changes done to <tt>x</tt> will also affect arr(the numpy array) and vice versa.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Using torch.from_numpy()
</span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="n">arr</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">=</span><span class="mi">100</span>
<span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([  0,   1, 100,   3,   4], dtype=torch.int32)
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Using torch.tensor()
</span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="n">arr</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">=</span><span class="mi">100</span>
<span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0, 1, 2, 3, 4], dtype=torch.int32)
</code></pre></div></div>

<h4>Creating special types of tensors</h4>

<p>Uninitialized tensors using <a href="https://pytorch.org/docs/stable/torch.html#torch.empty"><strong>torch.empty()</strong></a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]])
</code></pre></div></div>

<p>Initialised with zeroes or ones using <a href="https://pytorch.org/docs/stable/torch.html#torch.zeros"><strong>torch.zeros()</strong></a> and <a href="https://pytorch.org/docs/stable/torch.html#torch.ones"><strong>torch.ones()</strong></a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Passing datatype is recommended but not compulsory
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0, 0, 0],
        [0, 0, 0],
        [0, 0, 0],
        [0, 0, 0]])
</code></pre></div></div>

<p>Tensors in a range using <a href="https://pytorch.org/docs/stable/torch.html#torch.arange"><strong>torch.arange(start,end,step)</strong></a> and <a href="https://pytorch.org/docs/stable/torch.html#torch.linspace"><strong>torch.linspace(start,end,number_of_elements)</strong></a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># 0 included, 50 excluded
</span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 0,  5],
        [10, 15],
        [20, 25],
        [30, 35],
        [40, 45]])
</code></pre></div></div>

<p>Tensor to create 12 linearly spaced elements between 0 and 50 both included</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> 
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 0.0000,  4.5455,  9.0909, 13.6364],
        [18.1818, 22.7273, 27.2727, 31.8182],
        [36.3636, 40.9091, 45.4545, 50.0000]])
</code></pre></div></div>

<p>A seed for random numbers can be set using torch.manual_seed().</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;torch._C.Generator at 0x1bb6e171030&gt;
</code></pre></div></div>

<p>Generating random tensors:</p>

<p>A tensor of shape (3, 4) with random numbers from a uniform distribution over [0, 1)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> 
<span class="n">x</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0.9712, 0.0742, 0.5130, 0.7472],
        [0.4507, 0.9223, 0.9148, 0.1624],
        [0.7780, 0.1663, 0.6665, 0.4992]])
</code></pre></div></div>

<p>A tensor with shape (3, 4) with numbers from the normal distribution with mean 0 and standard deviation 1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> 
<span class="n">x</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 0.5252,  2.0810,  1.5700, -0.1474],
        [-0.2024,  0.4377,  1.1986,  0.7179],
        [-0.4969,  0.8618, -0.2603, -1.1157]])
</code></pre></div></div>

<p>A tensor of shape (3, 4) with random integers between 0 and 10</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span> 
<span class="n">x</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[7, 7, 1, 4],
        [7, 4, 9, 5],
        [1, 2, 5, 6]])
</code></pre></div></div>

<p>Instead of specifying the sizes of the tensors, we can use three other functions which serve the same purpose as above nute take in other tensors as inputs and return tensors of their shapes. Just suffix _like to the above functions as below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> 
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
<span class="n">y</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0.3099, 0.0135, 0.2955, 0.8752],
        [0.7608, 0.7589, 0.2097, 0.4063],
        [0.6469, 0.3655, 0.3926, 0.6284]])
</code></pre></div></div>

<p>Similarly <tt>torch.randn_like(x), torch.randint_like(0,10,x), torch.zeros_like(x)</tt> and <tt>torch.ones_like(x)</tt> may also be used.</p>

<h2>Operations on Tensors</h2>

<p>Now we will look at a few basic operations on Tensors. Indexing and slicing tensors are very similar to those of python lists. We shall look at a few examples</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0, 1],
        [2, 3],
        [4, 5]])
</code></pre></div></div>

<p>To get the left column:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0, 2, 4])
</code></pre></div></div>

<p>To get the left column as a (3,1) slice:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">[:,:</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0],
        [2],
        [4]])
</code></pre></div></div>

<p><strong>Reshaping a Tensor</strong></p>

<p>Two functions are generally used for reshaping tensor which are .view() and .reshape(). Both functions return a reshaped tensor without affecting the original tensor.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">x</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) &lt;br&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0, 1],
        [2, 3],
        [4, 5],
        [6, 7],
        [8, 9]])
</code></pre></div></div>

<p>However, we see that x is unchanged</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> 
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0, 1],
        [2, 3],
        [4, 5],
        [6, 7],
        [8, 9]]) &lt;br&gt;
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="c1"># Unchanged
</span></code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
</code></pre></div></div>

<p>To change the original tensor use <tt>x = x.reshape(2, 5)</tt></p>

<p>While .view() returns a tensor which shares storage with the original tensor, .reshape() may return a copy or a view of the original tensor. It (.reshape()) may or may not share the storage with the original tensor. Also .reshape() may act on <a href="https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays/26999092#26999092">contiguous</a> or non contiguous tensors, while .view can act only on contiguous tensors.</p>

<p>We can also infer the correct value for shape from the tensor by passing -1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0, 1, 2, 3, 4],
        [5, 6, 7, 8, 9]])
</code></pre></div></div>

<p>Also as seen before, we can suffix the function with ‘_as’ to pass in a tensor whose shape, we want to reshape the original tensor to.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 0,  2,  4,  6,  8],
        [10, 12, 14, 16, 18]])
</code></pre></div></div>

<p><strong>Other Basic Operations</strong></p>

<p>I will demonstrate the use of basic operations using the torch.add() function. This may be extended to other functions also.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([4., 6.])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([4., 6.])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">c</span><span class="p">)</span> <span class="c1">#Equivalent to c = torch.add(a, b)
</span></code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([4., 6.])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="c1">#Equivalent tp a = torch.add(a,b)
</span><span class="n">a</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([4., 6.])
</code></pre></div></div>

<p>The above can be extended to all basic arithmetic operations. Now we will look at a few more operations</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Multiplication (element-wise)
</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([3., 8.])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Dot Product
</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(11.)
</code></pre></div></div>

<p>Now let us see matrix multiplication. Normal matrix multiplication (that we know of) can be done using <strong>torch.mm()</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">],[</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">float</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'a: '</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">'b: '</span><span class="p">,</span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">'a x b: '</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>a:  torch.Size([2, 3])
b:  torch.Size([3, 2])
a x b:  torch.Size([2, 2])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[56., 62.],
        [80., 89.]])
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">@</span> <span class="n">b</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[56., 62.],
        [80., 89.]])
</code></pre></div></div>

<p>Matrix multiplication can also be done with boradcasting using the <tt>torch.matmul()</tt> or <tt>@</tt> operator. Click <a href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics">here</a> for more details on broadcasting.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="k">print</span><span class="p">((</span><span class="n">a</span> <span class="o">@</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([2, 3, 5])
torch.Size([2, 3, 5])
</code></pre></div></div>

<p>But we see that there matrices are invalid for normal matrix multiplication</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">))</span> 
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-93-244d2942b50e&gt; in &lt;module&gt;
----&gt; 1 print(torch.mm(a,b))


RuntimeError: matrices expected, got 3D, 2D tensors at ..\aten\src\TH/generic/THTensorMath.cpp:956
</code></pre></div></div>

<p>Note that if the tensors satisfy the mathematical conditions of matric multiplication, then all the three above functions will be identical. It is however easier to detect errors using <tt>torch.mm()</tt> than the other two when the tensors are mathematically non-compatible and hence is recommended over the other two.</p>

<p><strong>Norm Function</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span><span class="mf">5.</span><span class="p">,</span><span class="mf">8.</span><span class="p">,</span><span class="mf">14.</span><span class="p">])</span>
<span class="n">x</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(17.)
</code></pre></div></div>

<h2>Gradients in PyTorch</h2>

<p>PyTorch provides a module called ‘autograd’ to calculate the gradients of tensors automatically. It basically keeps track of all operations done on the tensor and backtracks along these operations to calculate gradients(or derivatives) along the way. To ensure that the operations are kept track of, we need to set the requires_grad attribute to True which can be done in two ways: (1) while creation, set the attribute to True as <tt>x = torch.arange(10, requires_grad = True)</tt> or (2) after creation, use <tt>x.requires_grad_(True)</tt>. The gradients are computed with respect to some variable y as <tt>y.backward()</tt>. This goes though all operations which were used to create y and calculates the gradients. For example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># Substitutes the value of x = 3 in the equation
</span></code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(21., grad_fn=&lt;AddBackward0&gt;)
</code></pre></div></div>

<p><img src="/blog/assets/img/intro-to-pytorch/s1.png" alt="Explanation" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Perform backpropagation on y to calculate gradients
</span><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>
<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Display the gradient wrt x
</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(12.)
</code></pre></div></div>

<p><img src="/blog/assets/img/intro-to-pytorch/s2.png" alt="Explanation" /></p>

<p><strong>Calculating multi-level gradients</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[1., 2., 3.],
        [3., 2., 1.]], requires_grad=True)
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">3</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[5., 7., 9.],
        [9., 7., 5.]], grad_fn=&lt;AddBackward0&gt;)
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">z</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[ 50.,  98., 162.],
        [162.,  98.,  50.]], grad_fn=&lt;MulBackward0&gt;)
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor(620., grad_fn=&lt;SumBackward0&gt;)
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>
<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[40., 56., 72.],
        [72., 56., 40.]])
</code></pre></div></div>

<p><img src="/blog/assets/img/intro-to-pytorch/s3.png" alt="Explanation" /></p>

<p><strong>Turning off tracking</strong></p>

<p>There may be times when we don’t want or need to track the computational history.</p>

<p>You can reset a tensor’s <tt>requires_grad</tt> attribute in-place using <tt>.requires_grad_(True)</tt> (or False) as needed.</p>

<p>When performing evaluations, it’s often helpful to wrap a set of operations in <tt>with torch.no_grad():</tt></p>

<p>A less-used method is to run <tt>.detach()</tt> on a tensor to prevent future computations from being tracked. This can be handy when cloning a tensor.</p>

<h2>Building a Simple Neural Network</h2>

<p>Note that the rest of the article will need some knowledge of Machine Learning or Neural Networks.<br />
We will discover other features like Dataloaders, Criterions and Optimizers using an example on the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">IRIS Dataset</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'iris.csv'</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>

<p>PyTorch has 2 really useful libraries for Neral Networks:<br /></p>
<ol>
  <li><tt>torch.nn</tt> generally imported as <tt>nn</tt> <br /></li>
  <li><tt>torch.nn.functional</tt> generally imported as <tt>F</tt></li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'target'</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'target'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># One Hot Encoding
</span><span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>  
<span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>
<p>To convert the training tensors into a dataset and to make things like batch gradient descent easier, we may use the <tt>TensorDataset</tt> and <tt>DataLoader</tt> classes.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trainset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<p>We will create batches of size 30. We will shuffle the training data so that the batches are not biased. However this is not necessary for the test data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trainloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">testloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<p>To access the batches:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 4 batches. Each batch is two-dimensional, one for the features(X) and other for the classes(y).
</span><span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">trainloader</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2
2
2
2
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># To access the index, X data and Y data
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0 tensor([[5.7000, 2.9000, 4.2000, 1.3000],
        [7.4000, 2.8000, 6.1000, 1.9000],
        [6.9000, 3.1000, 5.1000, 2.3000],
        [5.3000, 3.7000, 1.5000, 0.2000],
        [5.4000, 3.0000, 4.5000, 1.5000],
        [5.5000, 4.2000, 1.4000, 0.2000],
        [6.1000, 2.6000, 5.6000, 1.4000],
        [5.5000, 2.6000, 4.4000, 1.2000],
        [6.0000, 3.4000, 4.5000, 1.6000],
        [5.7000, 2.5000, 5.0000, 2.0000],
        [4.4000, 3.2000, 1.3000, 0.2000],
        [5.0000, 3.6000, 1.4000, 0.2000],
        [4.9000, 3.1000, 1.5000, 0.1000],
        [7.0000, 3.2000, 4.7000, 1.4000],
        [6.3000, 2.5000, 4.9000, 1.5000],
        [6.1000, 2.8000, 4.0000, 1.3000],
        [6.5000, 3.0000, 5.8000, 2.2000],
        [5.1000, 3.8000, 1.5000, 0.3000],
        [5.0000, 3.0000, 1.6000, 0.2000],
        [4.6000, 3.6000, 1.0000, 0.2000],
        [5.7000, 2.8000, 4.5000, 1.3000],
        [4.9000, 3.1000, 1.5000, 0.1000],
        [5.8000, 2.7000, 4.1000, 1.0000],
        [5.1000, 3.3000, 1.7000, 0.5000],
        [4.4000, 2.9000, 1.4000, 0.2000],
        [6.2000, 2.2000, 4.5000, 1.5000],
        [7.9000, 3.8000, 6.4000, 2.0000],
        [7.7000, 3.8000, 6.7000, 2.2000],
        [6.3000, 2.9000, 5.6000, 1.8000],
        [5.5000, 2.5000, 4.0000, 1.3000]]) tensor([1, 2, 2, 0, 1, 0, 2, 1, 1, 2, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 1, 0, 1, 0,
        0, 1, 2, 2, 2, 1])

1 tensor([[5.0000, 3.2000, 1.2000, 0.2000],
        [4.9000, 3.1000, 1.5000, 0.1000],
        [6.7000, 3.3000, 5.7000, 2.1000],
        [5.5000, 3.5000, 1.3000, 0.2000],
        [5.4000, 3.9000, 1.7000, 0.4000],
        [6.0000, 3.0000, 4.8000, 1.8000],
        [5.1000, 2.5000, 3.0000, 1.1000],
        [5.9000, 3.0000, 4.2000, 1.5000],
        [7.6000, 3.0000, 6.6000, 2.1000],
        [6.4000, 2.7000, 5.3000, 1.9000],
        [5.1000, 3.8000, 1.6000, 0.2000],
        [6.9000, 3.1000, 5.4000, 2.1000],
        [7.2000, 3.6000, 6.1000, 2.5000],
        [5.1000, 3.5000, 1.4000, 0.2000],
        [6.5000, 3.2000, 5.1000, 2.0000],
        [5.5000, 2.4000, 3.7000, 1.0000],
        [5.6000, 2.8000, 4.9000, 2.0000],
        [6.3000, 3.4000, 5.6000, 2.4000],
        [7.3000, 2.9000, 6.3000, 1.8000],
        [5.9000, 3.2000, 4.8000, 1.8000],
        [6.8000, 2.8000, 4.8000, 1.4000],
        [4.9000, 2.5000, 4.5000, 1.7000],
        [5.1000, 3.5000, 1.4000, 0.3000],
        [6.2000, 3.4000, 5.4000, 2.3000],
        [5.7000, 2.8000, 4.1000, 1.3000],
        [6.1000, 3.0000, 4.9000, 1.8000],
        [5.5000, 2.4000, 3.8000, 1.1000],
        [5.7000, 2.6000, 3.5000, 1.0000],
        [5.0000, 3.5000, 1.6000, 0.6000],
        [5.6000, 2.7000, 4.2000, 1.3000]]) tensor([0, 0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 2, 2, 0, 2, 1, 2, 2, 2, 1, 1, 2, 0, 2,
        1, 2, 1, 1, 0, 1])

2 tensor([[5.0000, 3.3000, 1.4000, 0.2000],
        [5.8000, 2.6000, 4.0000, 1.2000],
        [5.6000, 3.0000, 4.1000, 1.3000],
        [5.0000, 2.0000, 3.5000, 1.0000],
        [6.4000, 2.9000, 4.3000, 1.3000],
        [5.1000, 3.8000, 1.9000, 0.4000],
        [5.6000, 2.9000, 3.6000, 1.3000],
        [6.7000, 3.1000, 4.4000, 1.4000],
        [6.1000, 3.0000, 4.6000, 1.4000],
        [4.5000, 2.3000, 1.3000, 0.3000],
        [6.7000, 3.1000, 5.6000, 2.4000],
        [5.7000, 3.8000, 1.7000, 0.3000],
        [4.8000, 3.1000, 1.6000, 0.2000],
        [6.5000, 2.8000, 4.6000, 1.5000],
        [6.0000, 2.2000, 5.0000, 1.5000],
        [6.5000, 3.0000, 5.2000, 2.0000],
        [6.3000, 3.3000, 6.0000, 2.5000],
        [4.9000, 2.4000, 3.3000, 1.0000],
        [7.1000, 3.0000, 5.9000, 2.1000],
        [4.4000, 3.0000, 1.3000, 0.2000],
        [6.4000, 3.2000, 4.5000, 1.5000],
        [5.0000, 2.3000, 3.3000, 1.0000],
        [6.7000, 3.1000, 4.7000, 1.5000],
        [5.4000, 3.7000, 1.5000, 0.2000],
        [6.3000, 3.3000, 4.7000, 1.6000],
        [5.1000, 3.7000, 1.5000, 0.4000],
        [6.1000, 2.9000, 4.7000, 1.4000],
        [5.2000, 2.7000, 3.9000, 1.4000],
        [5.1000, 3.4000, 1.5000, 0.2000],
        [4.8000, 3.4000, 1.9000, 0.2000]]) tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 0, 0, 1, 2, 2, 2, 1, 2, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 0, 0])

3 tensor([[4.3000, 3.0000, 1.1000, 0.1000],
        [6.4000, 3.1000, 5.5000, 1.8000],
        [6.9000, 3.1000, 4.9000, 1.5000],
        [5.6000, 3.0000, 4.5000, 1.5000],
        [6.0000, 2.9000, 4.5000, 1.5000],
        [7.2000, 3.0000, 5.8000, 1.6000],
        [6.6000, 2.9000, 4.6000, 1.3000],
        [5.8000, 2.7000, 5.1000, 1.9000],
        [5.0000, 3.4000, 1.5000, 0.2000],
        [6.3000, 2.8000, 5.1000, 1.5000],
        [6.2000, 2.8000, 4.8000, 1.8000],
        [4.7000, 3.2000, 1.3000, 0.2000],
        [5.7000, 3.0000, 4.2000, 1.2000],
        [4.6000, 3.1000, 1.5000, 0.2000],
        [4.6000, 3.2000, 1.4000, 0.2000],
        [6.7000, 2.5000, 5.8000, 1.8000],
        [5.8000, 2.7000, 3.9000, 1.2000],
        [4.6000, 3.4000, 1.4000, 0.3000],
        [6.3000, 2.3000, 4.4000, 1.3000],
        [6.0000, 2.7000, 5.1000, 1.6000],
        [5.2000, 3.5000, 1.5000, 0.2000],
        [5.5000, 2.3000, 4.0000, 1.3000],
        [5.8000, 2.8000, 5.1000, 2.4000],
        [5.8000, 4.0000, 1.2000, 0.2000],
        [4.8000, 3.0000, 1.4000, 0.1000],
        [6.4000, 2.8000, 5.6000, 2.2000],
        [6.8000, 3.2000, 5.9000, 2.3000],
        [5.8000, 2.7000, 5.1000, 1.9000],
        [6.7000, 3.3000, 5.7000, 2.5000],
        [5.4000, 3.9000, 1.3000, 0.4000]]) tensor([0, 2, 1, 1, 1, 2, 1, 2, 0, 2, 2, 0, 1, 0, 0, 2, 1, 0, 1, 1, 0, 1, 2, 0,
        0, 2, 2, 2, 2, 0])
</code></pre></div></div>

<p><strong>Creating the Model Class</strong><br />
To define a Neural Network, we need to define a class which inherits from the <tt>nn.Module</tt> class. Here is where we can define all the layers, activation functions, embeddings etc. Here we will create a simple model with 2 hidden layers:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">h1</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">h2</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span><span class="n">h1</span><span class="p">)</span>    <span class="c1"># input layer
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">)</span>            <span class="c1"># hidden layer
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">h2</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>  <span class="c1"># output layer
</span>        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Define the activation functions for the layers
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p><br /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Instantiate the Model class using parameter defaults:
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>Defining the loss function and optimizer:</strong><br />
The loss function is conventionally defined as <strong>criterion</strong>. THe various loss functions are available in <tt>torch.nn</tt> library and the optimizers are available in the <tt>torch.optim</tt> library. Here we will use Cross Entropy Loss and the Adam Optimizer</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span> <span class="c1"># lr: Learning Rate
</span></code></pre></div></div>

<p><strong>Training the model</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">i</span><span class="o">+=</span><span class="mi">1</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
        
        
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    
    
    <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="mi">10</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'epoch: {i:2}  loss: {loss.item():10.8f}'</span><span class="p">)</span>


</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>epoch:  1  loss: 1.09409952
epoch: 11  loss: 0.96279073
epoch: 21  loss: 0.87627226
epoch: 31  loss: 0.72676194
epoch: 41  loss: 0.66376936
epoch: 51  loss: 0.68377405
epoch: 61  loss: 0.68355846
epoch: 71  loss: 0.64891487
epoch: 81  loss: 0.66468638
epoch: 91  loss: 0.66344351
</code></pre></div></div>

<p>Above, since the backward() function accumulates gradients, to avoid mixing up of gradients between minibatches, you have to zero them out beore backpropagating on the next batch. <tt>optimizer.zero_grad()</tt> is used for this. <tt>optimizer.step</tt>  performs a parameter update based on the current gradient (stored in <tt>.grad</tt> attribute of a parameter) and the update rule</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="o">*</span><span class="mi">4</span><span class="p">),</span> <span class="n">losses</span><span class="p">)</span> <span class="c1">#epochs*4 because each epoch is made of 4 batches and for each of them a loss is calculated
</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Loss'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'epoch'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/blog/assets/img/intro-to-pytorch/graph.png" alt="graph" /></p>

<p><strong>Saving the Model’s Parameters</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s">'IrisDatasetModel.pt'</span><span class="p">)</span>
</code></pre></div></div>

<p>Only the parameters of the model are saved and not the model itself. 
For more information on saving and loading visit https://pytorch.org/tutorials/beginner/saving_loading_models.html</p>

<p><strong>Loading a Model</strong></p>

<p>We’ll load a new model object and test it as we had before to make sure it worked.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="n">new_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'IrisDatasetModel.pt'</span><span class="p">))</span>
<span class="n">new_model</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model(
  (fc1): Linear(in_features=4, out_features=10, bias=True)
  (fc2): Linear(in_features=10, out_features=10, bias=True)
  (out): Linear(in_features=10, out_features=3, bias=True)
)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">y_val</span> <span class="o">=</span> <span class="n">new_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_val</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'{loss:.8f}'</span><span class="p">)</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.69291312
</code></pre></div></div>

<p>References:</p>
<ol>
  <li>https://pytorch.org/docs/stable/index.html</li>
  <li>https://www.udacity.com/course/deep-learning-pytorch–ud188</li>
  <li>https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch</li>
  <li>https://en.wikipedia.org/wiki/PyTorch</li>
  <li>https://www.analyticsvidhya.com/blog/2018/02/pytorch-tutorial/</li>
  <li>https://www.analyticsvidhya.com/blog/2019/09/introduction-to-pytorch-from-scratch/</li>
</ol>

<p><a href="https://colab.research.google.com/drive/1odpZAS42UzVk16TGol_ta24DtyX1ZB2B"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></p>

<hr />
:ET